{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>"},{"location":"#introduction-to-snakemake","title":"Introduction to Snakemake","text":"<p>Automate Your Workflow With Snakemake</p> <p>Are you working with big data?</p> <p>Do you need to pass your data through various software?</p> <p>Oh wait! Have I updated this output file?</p> <p>If you\u2019ve ever been in this situation, you would know that it can become quite difficult to maintain consistency and accuracy.</p> <p>The more manual steps we execute, the more human errors that are inevitably introduced into our analysis - hampering accuracy and reproducibility.</p> <p>Sit back and let the machines do their magic.</p> <p>Workflow languages automate your data analysis workflow. They also ensure that all your analysis logs are captured in an organized fashion, explicitly outline the software used, capture the input and output files at each step and even allow you to restart the pipeline from where it errored out. The process ensures higher productivity and decreases loss of resources re-running your workflow from the start. Additionally, when your data inevitably becomes big data, workflow languages allow you to easily scale up - meaning, you can move your analysis to a high performance cluster (HPC) without stress!</p> <p>In this hands-on workshop,We will guide you through an introduction to Snakemake, a workflow language with its basis in the popular programming language, Python. Attendees can expect to learn:</p> <ul> <li>The benefits of using Snakemake or other workflow languages,</li> <li>How to create a workflow to organize your computations, and</li> <li>How an HPC scheduler (such as Slurm) fits into your workflow</li> </ul> <p>Who should attend</p> <p>This Workshop is intended for anyone who has several steps in their data analysis workflow. It is a hands-on workshop, meaning you will be coding along with Leah and the NeSI team.</p> <p>If you have any questions or would like more information about this session, please email training@nesi.org.nz</p> <p>Attribution notice</p> <ul> <li>Material used in this workshop is based on the following repository<ul> <li>Author     : Leah Kemp (ESR)</li> <li>Repository : https://leahkemp.github.io/RezBaz2020_snakemake_workshop/</li> </ul> </li> </ul> <p>Setup</p> <p>This is an online workshop that will use Jupyter on NeSI. Attendees will be required to set up an account on NeSI in order to participate. Full instructions will be sent to registrants closer to the time of the workshop.</p> <p>Workshop</p> <p>Workshop sections:</p> <ul> <li>01 - Introduction</li> <li>02 - Setup</li> <li>03 - Create a basic workflow</li> <li>04 - Leveling up your workflow</li> <li>05 - We want more!</li> </ul> <p>The workflows we will create:</p> <ul> <li>Basic demo workflow</li> <li>Leveled up demo workflow</li> </ul> <p>Appendices:</p> <ul> <li>Setup for running on your machine using conda</li> <li>Use conda environments</li> </ul>"},{"location":"workshop_material/01_introduction/","title":"01 - Introduction","text":""},{"location":"workshop_material/01_introduction/#benefits-of-workflow-languages","title":"Benefits of workflow languages","text":"<ul> <li>Reproducibility</li> <li>Automation</li> <li>Portability</li> <li>Scalability</li> <li>Interpretability</li> </ul> <p>A good paper on Scalable Workflows and Reproducible Data Analysis for Genomics - although genomics focussed, it covers a lot of the concepts we will touch on in this workshop</p>"},{"location":"workshop_material/01_introduction/#benefits-of-snakemake","title":"Benefits of Snakemake","text":"<ul> <li>Based in the popular (and widely used) programming language, Python</li> <li>Great documentation, actively maintained (but so are the other workflow languages mentioned below)</li> <li>Easier to learn (particularly if you're familiar with python)</li> <li>Flexible</li> </ul> <p>See what other people think:</p> <ul> <li>https://www.biostars.org/p/345998/</li> <li>https://www.reddit.com/r/bioinformatics/comments/f3rxel/snakemake_vs_nextflow/</li> <li>https://www.biostars.org/p/258436/</li> </ul>"},{"location":"workshop_material/01_introduction/#other-workflow-languages","title":"Other workflow languages","text":"<p>Choose your favourite flavour of workflow language!</p> <ul> <li>Common Workflow Language (CWL)</li> <li>Nextflow</li> <li>Workflow Description Language (WDL)</li> <li>Guix Workflow Language (GWL)</li> </ul> <p>The real point is to use a workflow language (where applicable) and just use the flavour you like!</p>"},{"location":"workshop_material/01_introduction/#this-workshop","title":"This workshop","text":"<p>This workshop is designed with someone who had some familiarity with the command line. However, I've tried to make it as accessible as possible to anyone who wants to learn Snakemake.</p> <p>Throughout this workshop, I'll be indicating the code to remove and the code to insert (relative to the previous step) with the following:</p> <pre><code>- Remove this line of code\n+ Add this line of code\n</code></pre> <p>However, the actual <code>+</code> and <code>-</code> symbols should not be included in your own code</p> <p>At each section of the workshop you can find a drop down box under \"Current snakefile:\" that will contain the main Snakefile that comprises the pipeline as a plain text file to copy and paste from if you need to catch up.</p> <p>Back to homepage</p>"},{"location":"workshop_material/02_setup/","title":"02 - Setup","text":""},{"location":"workshop_material/02_setup/#running-on-nesi-vs-on-your-computer","title":"Running on NeSI vs on your computer","text":"<p>During this workshop we will be running the material on the NeSI platform, using the Jupyter interface, however it is also possible to run this material locally on your own machine. </p> <p>One of the differences between running on NeSI or your own machine is that on NeSI we preinstall popular software and make it available to our users, whereas on your own machine you need to install the software yourself (e.g. using a package manager such as conda).</p> <p>We have provided a guide for setting up your own machine using conda here) (note we will not be able to provide assistance if you decide to take this approach during the workshop).</p> Connect to Jupyter on NeSI <ol> <li>Connect to https://jupyter.nesi.org.nz</li> <li><p>Enter NeSI username, HPC password and 6 digit second factor token (as set on MyNeSI)</p></li> <li><p>Choose server options as below make sure to choose the correct project code <code>nesi99991</code>, number of CPUs 4, memory 4GB prior to pressing  button. <p></p> <ol> <li><p>Start a terminal session from the JupyterLab launcher"},{"location":"workshop_material/02_setup/#create-a-working-directory","title":"Create a working directory","text":"<p>When you connect to NeSI JupyterLab you always start in a new hidden directory. To make sure you can find your work next time, you should change to another location. Here we will switch to our project directory, since home directories can run out of space quickly. If you are using your own project use that instead of \"nesi99991\".</p> <p>code</p> <p><pre><code>mkdir -p /nesi/project/nesi99991/snakemake20220512/$USER\n</code></pre> <pre><code>cd /nesi/project/nesi99991/snakemake20220512/$USER\n</code></pre></p> <p>You can also navigate to the above directory in the JupyterLab file browser, which can be useful for editing files and viewing images and html documents.</p>"},{"location":"workshop_material/02_setup/#load-the-snakemake-module","title":"Load the Snakemake module","text":"<p>We use \"environment modules\" on NeSI to manage installed software. This allows you to pick and choose which software is available in your environment.  More details about environment modules can be found on the NeSI support page.</p> <p>The JupyterLab terminal comes with some modules preloaded and it can often be nicer to start with a clean environment:</p> <pre><code>module purge\n</code></pre> <p>We can search for available Snakemake modules using the <code>module spider</code> command:</p> <pre><code>module spider snakemake\n</code></pre> <p>which shows we have many versions of snakemake installed. Now load a specific version of snakemake into your environment:</p> <pre><code>module load snakemake/7.6.2-gimkl-2020a-Python-3.9.9\n</code></pre> <p>Test that the snakemake command is now available by running the following command:</p> <pre><code>snakemake --version\n</code></pre> <p>It should print out the version of snakemake, i.e. \"7.6.2\".</p> <p>You can also run <code>module list</code> to see the list of modules that are currently loaded.</p>"},{"location":"workshop_material/02_setup/#clone-this-repo","title":"Clone this repo","text":"<p>Clone this repo with the following:</p> <p>code</p> <p><pre><code>git clone https://github.com/nesi/snakemake_workshop.git\n</code></pre> <pre><code>cd snakemake_workshop\n</code></pre></p> <p>See the Git Guides for information on cloning a repo</p> <p>Back to homepage</p>"},{"location":"workshop_material/03_create_a_basic_workflow/","title":"03 - Create a basic workflow","text":""},{"location":"workshop_material/03_create_a_basic_workflow/#301-aim","title":"3.01 Aim","text":"<p>Let's create a basic workflow that will do some of the analysis steps for genetic data. We will have three samples with two files each - six files in total. These files will be processed through the below workflow, passing through three software.</p> <p> </p> <p>We have paired end sequencing data for three samples <code>NA24631</code> to process in the <code>./data</code> directory. Let's have a look:</p> <p>code</p> <pre><code>ls -lh ./data/\n</code></pre> output <pre><code>total 13M\n-rw-rw----+ 1 lkemp nesi99991 2.1M May 11 12:06 NA24631_1.fastq.gz\n-rw-rw----+ 1 lkemp nesi99991 2.3M May 11 12:06 NA24631_2.fastq.gz\n-rw-rw----+ 1 lkemp nesi99991 2.1M May 11 12:06 NA24694_1.fastq.gz\n-rw-rw----+ 1 lkemp nesi99991 2.3M May 11 12:06 NA24694_2.fastq.gz\n-rw-rw----+ 1 lkemp nesi99991 1.8M May 11 12:06 NA24695_1.fastq.gz\n-rw-rw----+ 1 lkemp nesi99991 1.9M May 11 12:06 NA24695_2.fastq.gz\n</code></pre> <p></p>"},{"location":"workshop_material/03_create_a_basic_workflow/#302-snakemake-workflow-file-structure","title":"3.02 Snakemake workflow file structure","text":"<p>Workflow file structure:</p> <pre><code>demo_workflow/\n      |_______results/\n      |_______workflow/\n                 |_______Snakefile\n</code></pre> <p>We will create and run our workflow from the <code>workflow</code> directory send all of our file outputs/results to the <code>results</code> directory</p> <p>Read up on the best practice workflow structure here</p> <p>Create this file structure and our main Snakefile with:</p> <p>code</p> <p><pre><code>mkdir -p demo_workflow/{results,workflow}\n</code></pre> <pre><code>touch demo_workflow/workflow/Snakefile\n</code></pre></p> <p>Now you should have the very beginnings of your Snakemake workflow in a <code>demo_workflow</code> directory. Let's have a look:</p> <p>code</p> <pre><code>ls -lh demo_workflow/\n</code></pre> output <pre><code>total 1.0K\ndrwxrws---+ 2 lkemp nesi99991 4.0K May 11 12:07 results\ndrwxrws---+ 2 lkemp nesi99991 4.0K May 11 12:07 workflow\n</code></pre> <p></p> <p>code</p> <pre><code>ls -lh demo_workflow/workflow/\n</code></pre> output <pre><code>total 0\n-rw-rw----+ 1 lkemp nesi99991 0 May 11 12:07 Snakefile\n</code></pre> <p></p> <p>Within the <code>workflow</code> directory (where we will create and run our workflow), we have a <code>Snakefile</code> file that will be the backbone of our workflow.</p>"},{"location":"workshop_material/03_create_a_basic_workflow/#303-run-the-software-on-the-command-line","title":"3.03 Run the software on the command line","text":"<p>First lets run the first step in our workflow (fastqc) directly on the command line to get the syntax of the command right and check what outputs files we expect to get. Knowing what files the software will output is important for Snakemake since it is a lazy \"pull\" based system where software/rules will only run if you tell it to create the output file. We will talk more about this later!</p> <p>code</p> <ul> <li> <p>First make sure to have fastqc available. On NeSI, load the corresponding module <pre><code>module load FastQC/0.11.9\n</code></pre></p> </li> <li> <p>See what parameters are available so we know how we want to run this software before we put it in a Snakemake workflow <pre><code>fastqc --help\n</code></pre></p> </li> <li> <p>Create a test directory to put the output files <pre><code>mkdir fastqc_test\n</code></pre></p> </li> </ul> <p>Run fastqc directly on the command line on one of the samples <pre><code>fastqc ./data/NA24631_1.fastq.gz ./data/NA24631_2.fastq.gz -o ./fastqc_test -t 2\n</code></pre></p> output <pre><code>Started analysis of NA24631_1.fastq.gz\nApprox 5% complete for NA24631_1.fastq.gz\nApprox 10% complete for NA24631_1.fastq.gz\nApprox 15% complete for NA24631_1.fastq.gz\nApprox 20% complete for NA24631_1.fastq.gz\nApprox 25% complete for NA24631_1.fastq.gz\nApprox 30% complete for NA24631_1.fastq.gz\nApprox 35% complete for NA24631_1.fastq.gz\nApprox 40% complete for NA24631_1.fastq.gz\nApprox 45% complete for NA24631_1.fastq.gz\nApprox 50% complete for NA24631_1.fastq.gz\nApprox 55% complete for NA24631_1.fastq.gz\nApprox 60% complete for NA24631_1.fastq.gz\nStarted analysis of NA24631_2.fastq.gz\nApprox 65% complete for NA24631_1.fastq.gz\nApprox 5% complete for NA24631_2.fastq.gz\nApprox 70% complete for NA24631_1.fastq.gz\nApprox 10% complete for NA24631_2.fastq.gz\nApprox 75% complete for NA24631_1.fastq.gz\nApprox 15% complete for NA24631_2.fastq.gz\nApprox 80% complete for NA24631_1.fastq.gz\nApprox 20% complete for NA24631_2.fastq.gz\nApprox 25% complete for NA24631_2.fastq.gz\nApprox 85% complete for NA24631_1.fastq.gz\nApprox 90% complete for NA24631_1.fastq.gz\nApprox 30% complete for NA24631_2.fastq.gz\nApprox 35% complete for NA24631_2.fastq.gz\nApprox 95% complete for NA24631_1.fastq.gz\nApprox 40% complete for NA24631_2.fastq.gz\nAnalysis complete for NA24631_1.fastq.gz\nApprox 45% complete for NA24631_2.fastq.gz\nApprox 50% complete for NA24631_2.fastq.gz\nApprox 55% complete for NA24631_2.fastq.gz\nApprox 60% complete for NA24631_2.fastq.gz\nApprox 65% complete for NA24631_2.fastq.gz\nApprox 70% complete for NA24631_2.fastq.gz\nApprox 75% complete for NA24631_2.fastq.gz\nApprox 80% complete for NA24631_2.fastq.gz\nApprox 85% complete for NA24631_2.fastq.gz\nApprox 90% complete for NA24631_2.fastq.gz\nApprox 95% complete for NA24631_2.fastq.gz\nAnalysis complete for NA24631_2.fastq.gz\n</code></pre> <p></p> <ul> <li>What are the output files of fastqc? Find out with: <pre><code>ls -lh ./fastqc_test\n</code></pre></li> </ul> output <pre><code>total 2.5M\n-rw-rw----+ 1 lkemp nesi99991 718K May 11 12:08 NA24631_1_fastqc.html\n-rw-rw----+ 1 lkemp nesi99991 475K May 11 12:08 NA24631_1_fastqc.zip\n-rw-rw----+ 1 lkemp nesi99991 726K May 11 12:08 NA24631_2_fastqc.html\n-rw-rw----+ 1 lkemp nesi99991 479K May 11 12:08 NA24631_2_fastqc.zip\n</code></pre> <p></p>"},{"location":"workshop_material/03_create_a_basic_workflow/#304-create-the-first-rule-in-your-workflow","title":"3.04 Create the first rule in your workflow","text":"<p>Let's wrap this up in a Snakemake workflow! Start with the basic structure of a Snakefile:</p> <pre><code># target OUTPUT files for the whole workflow\nrule all:\ninput:\n# workflow\nrule my_rule:\ninput:\n\"\"\noutput:\n\"\"\nthreads:\nshell:\n\"\"\n</code></pre> <p>Now add our fastqc rule, let's:</p> <ul> <li>Name the rule</li> <li>Fill in the the input fastq files from the <code>data</code> directory (path relative to the Snakefile)</li> <li>Fill in the output files (now you can see it's useful to know what files fastqc outputs!)</li> <li>Set the number of threads</li> <li>Write the fastqc shell command in the <code>shell:</code> section and pass the input/output variables to the shell command</li> <li>Set the final output files for the whole workflow in <code>rule all:</code></li> </ul> <p>The use of the word <code>input</code> in <code>rule all</code> can be confusing, but in this context, it is referring to the final output files of the whole workflow</p> Edit snakefile <pre><code># target OUTPUT files for the whole workflow\nrule all:\n    input:\n+       \"../results/fastqc/NA24631_1_fastqc.html\",\n+       \"../results/fastqc/NA24631_2_fastqc.html\",\n+       \"../results/fastqc/NA24631_1_fastqc.zip\",\n+       \"../results/fastqc/NA24631_2_fastqc.zip\"\n# workflow\n- rule my_rule:\n+ rule fastqc:\n   input:\n+       R1 = \"../../data/NA24631_1.fastq.gz\",\n+       R2 = \"../../data/NA24631_2.fastq.gz\"\n   output:\n+       html = [\"../results/fastqc/NA24631_1_fastqc.html\", \"../results/fastqc/NA24631_2_fastqc.html\"],\n+       zip = [\"../results/fastqc/NA24631_1_fastqc.zip\", \"../results/fastqc/NA24631_2_fastqc.zip\"]\n+   threads: 2\n   shell:\n+       \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads}\"\n</code></pre> Current snakefile: <pre><code># target OUTPUT files for the whole workflow\nrule all:\ninput:\n\"../results/fastqc/NA24631_1_fastqc.html\",\n\"../results/fastqc/NA24631_2_fastqc.html\",\n\"../results/fastqc/NA24631_1_fastqc.zip\",\n\"../results/fastqc/NA24631_2_fastqc.zip\"\n# workflow\nrule fastqc:\ninput:\nR1 = \"../../data/NA24631_1.fastq.gz\",\nR2 = \"../../data/NA24631_2.fastq.gz\"\noutput:\nhtml = [\"../results/fastqc/NA24631_1_fastqc.html\", \"../results/fastqc/NA24631_2_fastqc.html\"],\nzip = [\"../results/fastqc/NA24631_1_fastqc.zip\", \"../results/fastqc/NA24631_2_fastqc.zip\"]\nthreads: 2\nshell:\n\"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads}\"\n</code></pre> <p></p> <p>When you have multiple input and output files:</p> <ul> <li>You can \"name\" you inputs/outputs, they can be called separately in the shell command</li> <li>Remember to use commas between multiple inputs/outputs, it's a common source of error!</li> </ul> <p>Let's test the workflow! First we need to be in the <code>workflow</code> directory, where the Snakefile is</p> <p>code</p> <pre><code>cd demo_workflow/workflow/\n</code></pre>"},{"location":"workshop_material/03_create_a_basic_workflow/#305-dryrun","title":"3.05 Dryrun","text":"<p>Then let's carry out a dryrun of the workflow, where no actual analysis is undertaken (fastqc is not run) but the overall Snakemake structure is run/validated. This is a good way to check for errors in your Snakemake workflow before actually running your workflow.</p> <p>code</p> <pre><code>snakemake --dryrun\n</code></pre> output <pre><code>Building DAG of jobs...\nJob stats:\njob       count    min threads    max threads\n------  -------  -------------  -------------\nall           1              1              1\nfastqc        1              1              1\ntotal         2              1              1\n[Wed May 11 12:09:56 2022]\nrule fastqc:\n    input: ../../data/NA24631_1.fastq.gz, ../../data/NA24631_2.fastq.gz\n    output: ../results/fastqc/NA24631_1_fastqc.html, ../results/fastqc/NA24631_2_fastqc.html, ../results/fastqc/NA24631_1_fastqc.zip, ../results/fastqc/NA24631_2_fastqc.zip\n    jobid: 1\nresources: tmpdir=/dev/shm/jobs/26763281\n\n[Wed May 11 12:09:56 2022]\nlocalrule all:\n    input: ../results/fastqc/NA24631_1_fastqc.html, ../results/fastqc/NA24631_2_fastqc.html, ../results/fastqc/NA24631_1_fastqc.zip, ../results/fastqc/NA24631_2_fastqc.zip\n    jobid: 0\nresources: tmpdir=/dev/shm/jobs/26763281\n\nJob stats:\njob       count    min threads    max threads\n------  -------  -------------  -------------\nall           1              1              1\nfastqc        1              1              1\ntotal         2              1              1\nThis was a dry-run (flag -n). The order of jobs does not reflect the order of execution.\n</code></pre> <p></p> <p>The last table in the output confirms that the workflow will run one sample (<code>count 1</code>) through fastqc (<code>job fastqc</code>)</p>"},{"location":"workshop_material/03_create_a_basic_workflow/#306-create-a-dag","title":"3.06 Create a DAG","text":"<p>We can also visualise our workflow by creating a directed acyclic graph (DAG). We can tell snakemake to create a DAG with the <code>--dag</code> flag, then pipe this output to the dot software and write the output to the file, <code>dag_1.png</code></p> <p>code</p> <pre><code>snakemake --dag | dot -Tpng &gt; dag_1.png\n</code></pre> DAG: <p></p> <p></p> <p>Our diagram has a node for each job which are connected by edges representing dependencies</p> <p>Note. this diagram can be output to several other image formats such as svg or pdf</p>"},{"location":"workshop_material/03_create_a_basic_workflow/#307-fullrun","title":"3.07 Fullrun","text":"<p>Let's do a full run of our workflow (by removing the <code>--dryrun</code> flag). We will also now need to specify the maximum number of cores to use at one time with the <code>--cores</code> flag before snakemake will run</p> <p>code</p> <pre><code>snakemake --cores 2\n</code></pre> output <pre><code>Building DAG of jobs...\nUsing shell: /usr/bin/bash\nProvided cores: 2\nRules claiming more threads will be scaled down.\nJob stats:\njob       count    min threads    max threads\n------  -------  -------------  -------------\nall           1              1              1\nfastqc        1              2              2\ntotal         2              1              2\nSelect jobs to execute...\n\n[Wed May 11 12:10:44 2022]\nrule fastqc:\n    input: ../../data/NA24631_1.fastq.gz, ../../data/NA24631_2.fastq.gz\n    output: ../results/fastqc/NA24631_1_fastqc.html, ../results/fastqc/NA24631_2_fastqc.html, ../results/fastqc/NA24631_1_fastqc.zip, ../results/fastqc/NA24631_2_fastqc.zip\n    jobid: 1\nthreads: 2\nresources: tmpdir=/dev/shm/jobs/26763281\n\nStarted analysis of NA24631_1.fastq.gz\nApprox 5% complete for NA24631_1.fastq.gz\nApprox 10% complete for NA24631_1.fastq.gz\nApprox 15% complete for NA24631_1.fastq.gz\nApprox 20% complete for NA24631_1.fastq.gz\nApprox 25% complete for NA24631_1.fastq.gz\nApprox 30% complete for NA24631_1.fastq.gz\nApprox 35% complete for NA24631_1.fastq.gz\nApprox 40% complete for NA24631_1.fastq.gz\nApprox 45% complete for NA24631_1.fastq.gz\nApprox 50% complete for NA24631_1.fastq.gz\nApprox 55% complete for NA24631_1.fastq.gz\nApprox 60% complete for NA24631_1.fastq.gz\nApprox 65% complete for NA24631_1.fastq.gz\nStarted analysis of NA24631_2.fastq.gz\nApprox 70% complete for NA24631_1.fastq.gz\nApprox 5% complete for NA24631_2.fastq.gz\nApprox 75% complete for NA24631_1.fastq.gz\nApprox 10% complete for NA24631_2.fastq.gz\nApprox 80% complete for NA24631_1.fastq.gz\nApprox 15% complete for NA24631_2.fastq.gz\nApprox 85% complete for NA24631_1.fastq.gz\nApprox 20% complete for NA24631_2.fastq.gz\nApprox 90% complete for NA24631_1.fastq.gz\nApprox 25% complete for NA24631_2.fastq.gz\nApprox 95% complete for NA24631_1.fastq.gz\nApprox 30% complete for NA24631_2.fastq.gz\nAnalysis complete for NA24631_1.fastq.gz\nApprox 35% complete for NA24631_2.fastq.gz\nApprox 40% complete for NA24631_2.fastq.gz\nApprox 45% complete for NA24631_2.fastq.gz\nApprox 50% complete for NA24631_2.fastq.gz\nApprox 55% complete for NA24631_2.fastq.gz\nApprox 60% complete for NA24631_2.fastq.gz\nApprox 65% complete for NA24631_2.fastq.gz\nApprox 70% complete for NA24631_2.fastq.gz\nApprox 75% complete for NA24631_2.fastq.gz\nApprox 80% complete for NA24631_2.fastq.gz\nApprox 85% complete for NA24631_2.fastq.gz\nApprox 90% complete for NA24631_2.fastq.gz\nApprox 95% complete for NA24631_2.fastq.gz\nAnalysis complete for NA24631_2.fastq.gz\n[Wed May 11 12:10:48 2022]\nFinished job 1.\n1 of 2 steps (50%) done\nSelect jobs to execute...\n\n[Wed May 11 12:10:48 2022]\nlocalrule all:\n    input: ../results/fastqc/NA24631_1_fastqc.html, ../results/fastqc/NA24631_2_fastqc.html, ../results/fastqc/NA24631_1_fastqc.zip, ../results/fastqc/NA24631_2_fastqc.zip\n    jobid: 0\nresources: tmpdir=/dev/shm/jobs/26763281\n\n[Wed May 11 12:10:48 2022]\nFinished job 0.\n2 of 2 steps (100%) done\nComplete log: .snakemake/log/2022-05-11T121044.745212.snakemake.log\n</code></pre> <p></p> <p>It worked! Now in our results directory we have our output files from fastqc. Let's have a look:</p> <p>code</p> <pre><code>ls -lh ../results/fastqc/\n</code></pre> output <pre><code>total 2.5M\n-rw-rw----+ 1 lkemp nesi99991 718K May 11 12:10 NA24631_1_fastqc.html\n-rw-rw----+ 1 lkemp nesi99991 475K May 11 12:10 NA24631_1_fastqc.zip\n-rw-rw----+ 1 lkemp nesi99991 726K May 11 12:10 NA24631_2_fastqc.html\n-rw-rw----+ 1 lkemp nesi99991 479K May 11 12:10 NA24631_2_fastqc.zip\n</code></pre> <p>{% include exercise.html title=\"e3dot10\" content=e3dot10%} </p>"},{"location":"workshop_material/03_create_a_basic_workflow/#308-lazy-evaluation","title":"3.08 Lazy evaluation","text":"<p>What happens if we try a dryrun or full run now?</p> <p>code</p> <pre><code>snakemake --dryrun --cores 2\n</code></pre> <p>output</p> <pre><code>Building DAG of jobs...\nNothing to be done (all requested files are present and up to date).\n</code></pre> <p></p> <p>code</p> <pre><code>snakemake --cores 2\n</code></pre> output <pre><code>Building DAG of jobs...\nNothing to be done (all requested files are present and up to date).\nComplete log: .snakemake/log/2022-05-11T121300.251492.snakemake.log\n</code></pre> <p></p> <p>Nothing happens, all the target files in <code>rule all</code> have already been created so Snakemake does nothing</p> <p>Also, what happens if we create another directed acyclic graph (DAG) after the workflow has been run?</p> <p>code</p> <pre><code>snakemake --dag | dot -Tpng &gt; dag_2.png\n</code></pre> DAG <p></p> <p></p> <p>Notice our workflow 'job nodes' are now dashed lines, this indicates that their output is up to date and therefore the rule doesn't need to be run. We already have our target files!</p> <p>This can be quite informative if your workflow errors out at a rule. You can visually check which rules successfully ran and which didn't.</p>"},{"location":"workshop_material/03_create_a_basic_workflow/#309-run-using-environment-modules","title":"3.09 Run using environment modules","text":"<p>fastqc worked because we loaded it in our current shell session. Let's specify the environment module for fastqc so the user of the workflow doesn't need to load it manually.</p> Edit snakefile \"Update our rule to use it using the <code>envmodules:</code> directive <pre><code># target OUTPUT files for the whole workflow\nrule all:\n    input:\n        \"../results/fastqc/NA24631_1_fastqc.html\",\n        \"../results/fastqc/NA24631_2_fastqc.html\",\n        \"../results/fastqc/NA24631_1_fastqc.zip\",\n        \"../results/fastqc/NA24631_2_fastqc.zip\"\n\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/NA24631_1.fastq.gz\",\n        R2 = \"../../data/NA24631_2.fastq.gz\"\n    output:\n        html = [\"../results/fastqc/NA24631_1_fastqc.html\", \"../results/fastqc/NA24631_2_fastqc.html\"],\n        zip = [\"../results/fastqc/NA24631_1_fastqc.zip\", \"../results/fastqc/NA24631_2_fastqc.zip\"]\n    threads: 2\n+   envmodules:\n+       \"FastQC/0.11.9\"\n   shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads}\"\n</code></pre> Current snakefile: <pre><code># target OUTPUT files for the whole workflow\nrule all:\ninput:\n\"../results/fastqc/NA24631_1_fastqc.html\",\n\"../results/fastqc/NA24631_2_fastqc.html\",\n\"../results/fastqc/NA24631_1_fastqc.zip\",\n\"../results/fastqc/NA24631_2_fastqc.zip\"\n# workflow\nrule fastqc:\ninput:\nR1 = \"../../data/NA24631_1.fastq.gz\",\nR2 = \"../../data/NA24631_2.fastq.gz\"\noutput:\nhtml = [\"../results/fastqc/NA24631_1_fastqc.html\", \"../results/fastqc/NA24631_2_fastqc.html\"],\nzip = [\"../results/fastqc/NA24631_1_fastqc.zip\", \"../results/fastqc/NA24631_2_fastqc.zip\"]\nthreads: 2\nenvmodules:\n\"FastQC/0.11.9\"\nshell:\n\"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads}\"\n</code></pre> <p></p> <p>Run again, now telling Snakemake to use environment modules to automatically load our software by using the <code>--use-envmodules</code> flag</p> <p>code</p> <pre><code># first remove output of last run\nrm -r ../results/*\n\n# Run dryrun again\n- snakemake --dryrun --cores 2\n+ snakemake --dryrun --cores 2 --use-envmodules\n</code></pre> output <pre><code>Building DAG of jobs...\nJob stats:\njob       count    min threads    max threads\n------  -------  -------------  -------------\nall           1              1              1\nfastqc        1              2              2\ntotal         2              1              2\n[Wed May 11 12:13:52 2022]\nrule fastqc:\n    input: ../../data/NA24631_1.fastq.gz, ../../data/NA24631_2.fastq.gz\n    output: ../results/fastqc/NA24631_1_fastqc.html, ../results/fastqc/NA24631_2_fastqc.html, ../results/fastqc/NA24631_1_fastqc.zip, ../results/fastqc/NA24631_2_fastqc.zip\n    jobid: 1\nthreads: 2\nresources: tmpdir=/dev/shm/jobs/26763281\n\n[Wed May 11 12:13:52 2022]\nlocalrule all:\n    input: ../results/fastqc/NA24631_1_fastqc.html, ../results/fastqc/NA24631_2_fastqc.html, ../results/fastqc/NA24631_1_fastqc.zip, ../results/fastqc/NA24631_2_fastqc.zip\n    jobid: 0\nresources: tmpdir=/dev/shm/jobs/26763281\n\nJob stats:\njob       count    min threads    max threads\n------  -------  -------------  -------------\nall           1              1              1\nfastqc        1              2              2\ntotal         2              1              2\nThis was a dry-run (flag -n). The order of jobs does not reflect the order of execution.\n</code></pre> <p></p> <p>Let's do a full run</p> <pre><code>- snakemake --cores 2\n+ snakemake --cores 2 --use-envmodules\n</code></pre> output <pre><code>Building DAG of jobs...\nUsing shell: /usr/bin/bash\nProvided cores: 2\nRules claiming more threads will be scaled down.\nJob stats:\njob       count    min threads    max threads\n------  -------  -------------  -------------\nall           1              1              1\nfastqc        1              2              2\ntotal         2              1              2\nSelect jobs to execute...\n\n[Wed May 11 12:14:22 2022]\nrule fastqc:\n    input: ../../data/NA24631_1.fastq.gz, ../../data/NA24631_2.fastq.gz\n    output: ../results/fastqc/NA24631_1_fastqc.html, ../results/fastqc/NA24631_2_fastqc.html, ../results/fastqc/NA24631_1_fastqc.zip, ../results/fastqc/NA24631_2_fastqc.zip\n    jobid: 1\nthreads: 2\nresources: tmpdir=/dev/shm/jobs/26763281\n\nActivating environment modules: FastQC/0.11.9\n\nThe following modules were not unloaded:\n   (Use \"module --force purge\" to unload all):\n\n1) XALT/minimal   2) slurm   3) NeSI\nStarted analysis of NA24631_1.fastq.gz\nApprox 5% complete for NA24631_1.fastq.gz\nApprox 10% complete for NA24631_1.fastq.gz\nApprox 15% complete for NA24631_1.fastq.gz\nApprox 20% complete for NA24631_1.fastq.gz\nApprox 25% complete for NA24631_1.fastq.gz\nApprox 30% complete for NA24631_1.fastq.gz\nApprox 35% complete for NA24631_1.fastq.gz\nApprox 40% complete for NA24631_1.fastq.gz\nApprox 45% complete for NA24631_1.fastq.gz\nApprox 50% complete for NA24631_1.fastq.gz\nApprox 55% complete for NA24631_1.fastq.gz\nApprox 60% complete for NA24631_1.fastq.gz\nApprox 65% complete for NA24631_1.fastq.gz\nApprox 70% complete for NA24631_1.fastq.gz\nApprox 75% complete for NA24631_1.fastq.gz\nStarted analysis of NA24631_2.fastq.gz\nApprox 5% complete for NA24631_2.fastq.gz\nApprox 80% complete for NA24631_1.fastq.gz\nApprox 10% complete for NA24631_2.fastq.gz\nApprox 85% complete for NA24631_1.fastq.gz\nApprox 15% complete for NA24631_2.fastq.gz\nApprox 90% complete for NA24631_1.fastq.gz\nApprox 20% complete for NA24631_2.fastq.gz\nApprox 95% complete for NA24631_1.fastq.gz\nApprox 25% complete for NA24631_2.fastq.gz\nAnalysis complete for NA24631_1.fastq.gz\nApprox 30% complete for NA24631_2.fastq.gz\nApprox 35% complete for NA24631_2.fastq.gz\nApprox 40% complete for NA24631_2.fastq.gz\nApprox 45% complete for NA24631_2.fastq.gz\nApprox 50% complete for NA24631_2.fastq.gz\nApprox 55% complete for NA24631_2.fastq.gz\nApprox 60% complete for NA24631_2.fastq.gz\nApprox 65% complete for NA24631_2.fastq.gz\nApprox 70% complete for NA24631_2.fastq.gz\nApprox 75% complete for NA24631_2.fastq.gz\nApprox 80% complete for NA24631_2.fastq.gz\nApprox 85% complete for NA24631_2.fastq.gz\nApprox 90% complete for NA24631_2.fastq.gz\nApprox 95% complete for NA24631_2.fastq.gz\nAnalysis complete for NA24631_2.fastq.gz\n[Wed May 11 12:14:26 2022]\nFinished job 1.\n1 of 2 steps (50%) done\nSelect jobs to execute...\n\n[Wed May 11 12:14:26 2022]\nlocalrule all:\n    input: ../results/fastqc/NA24631_1_fastqc.html, ../results/fastqc/NA24631_2_fastqc.html, ../results/fastqc/NA24631_1_fastqc.zip, ../results/fastqc/NA24631_2_fastqc.zip\n    jobid: 0\nresources: tmpdir=/dev/shm/jobs/26763281\n\n[Wed May 11 12:14:26 2022]\nFinished job 0.\n2 of 2 steps (100%) done\nComplete log: .snakemake/log/2022-05-11T121422.744466.snakemake.log\n</code></pre> <p></p> <p>Notice it now says that \"Activating environment modules: FastQC/0.11.9\". Now the software our workflow uses will be automatically loaded!</p>"},{"location":"workshop_material/03_create_a_basic_workflow/#310-capture-our-logs","title":"3.10 Capture our logs","text":"<p>So far our logs (for fastqc) have been simply printed to our screen. As you can imagine, if you had a large automated workflow (that you might not be sitting at the computer watching run) you'll want to capture all that information. Therefore, any information the software spits out (including error messages!) will be kept and can be looked at once you return to your machine from your coffee break.</p> <p>We can get the logs for each rule to be written to a log file via the <code>log:</code> directive:</p> <ul> <li>It's a good idea to organise the logs by:</li> <li>Putting the logs in a directory labelled after the rule/software that was run</li> <li> <p>Labelling the log files with the sample name the software was run on</p> </li> <li> <p>Also make sure you tell the software (fastqc) to write the standard output and standard error to this log file we defined in the <code>log:</code> directive in the shell script (eg. <code>&amp;&gt; {log}</code>)</p> </li> </ul> Edit snakefile <pre><code># target OUTPUT files for the whole workflow\nrule all:\n    input:\n        \"../results/fastqc/NA24631_1_fastqc.html\",\n        \"../results/fastqc/NA24631_2_fastqc.html\",\n        \"../results/fastqc/NA24631_1_fastqc.zip\",\n        \"../results/fastqc/NA24631_2_fastqc.zip\"\n\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/NA24631_1.fastq.gz\",\n        R2 = \"../../data/NA24631_2.fastq.gz\"\n    output:\n        html = [\"../results/fastqc/NA24631_1_fastqc.html\", \"../results/fastqc/NA24631_2_fastqc.html\"],\n        zip = [\"../results/fastqc/NA24631_1_fastqc.zip\", \"../results/fastqc/NA24631_2_fastqc.zip\"]\n+   log:\n+       \"logs/fastqc/NA24631.log\"\n   threads: 2\n    envmodules:\n        \"FastQC/0.11.9\"\n    shell:\n-       \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads}\"\n+       \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} &amp;&gt; {log}\"\n</code></pre> Current snakefile <pre><code># target OUTPUT files for the whole workflow\nrule all:\ninput:\n\"../results/fastqc/NA24631_1_fastqc.html\",\n\"../results/fastqc/NA24631_2_fastqc.html\",\n\"../results/fastqc/NA24631_1_fastqc.zip\",\n\"../results/fastqc/NA24631_2_fastqc.zip\"\n# workflow\nrule fastqc:\ninput:\nR1 = \"../../data/NA24631_1.fastq.gz\",\nR2 = \"../../data/NA24631_2.fastq.gz\"\noutput:\nhtml = [\"../results/fastqc/NA24631_1_fastqc.html\", \"../results/fastqc/NA24631_2_fastqc.html\"],\nzip = [\"../results/fastqc/NA24631_1_fastqc.zip\", \"../results/fastqc/NA24631_2_fastqc.zip\"]\nlog:\n\"logs/fastqc/NA24631.log\"\nthreads: 2\nenvmodules:\n\"FastQC/0.11.9\"\nshell:\n\"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} &amp;&gt; {log}\"\n</code></pre> <p></p> <p>A tangent about standard streams</p> <ul> <li>These are standard streams in which information is returned by a computer process - in our case the logs that we see returned to us on our screen when we run fastqc</li> <li>There are two main streams:</li> <li>standard output (the log messages)</li> <li>standard error (the error messages)</li> </ul> <p>Different ways to write log files:</p> Syntax standard output in terminal standard error in terminal standard output in file standard error in file <code>&gt;</code> NO YES YES NO <code>2&gt;</code> YES NO NO YES <code>&amp;&gt;</code> NO NO YES YES <p>(Table adapted from here)</p> <p>Run again</p> <p><pre><code># remove output of last run\nrm -r ../results/*\n</code></pre> <pre><code># run dryrun/run again\nsnakemake --dryrun --cores 2 --use-envmodules\nsnakemake --cores 2 --use-envmodules\n</code></pre></p> output <pre><code>Building DAG of jobs...\nUsing shell: /usr/bin/bash\nProvided cores: 2\nRules claiming more threads will be scaled down.\nJob stats:\njob       count    min threads    max threads\n------  -------  -------------  -------------\nall           1              1              1\nfastqc        1              2              2\ntotal         2              1              2\nSelect jobs to execute...\n\n[Wed May 11 12:15:16 2022]\nrule fastqc:\n    input: ../../data/NA24631_1.fastq.gz, ../../data/NA24631_2.fastq.gz\n    output: ../results/fastqc/NA24631_1_fastqc.html, ../results/fastqc/NA24631_2_fastqc.html, ../results/fastqc/NA24631_1_fastqc.zip, ../results/fastqc/NA24631_2_fastqc.zip\n    log: logs/fastqc/NA24631.log\n    jobid: 1\nthreads: 2\nresources: tmpdir=/dev/shm/jobs/26763281\n\nActivating environment modules: FastQC/0.11.9\n\nThe following modules were not unloaded:\n   (Use \"module --force purge\" to unload all):\n\n1) XALT/minimal   2) slurm   3) NeSI\n[Wed May 11 12:15:20 2022]\nFinished job 1.\n1 of 2 steps (50%) done\nSelect jobs to execute...\n\n[Wed May 11 12:15:20 2022]\nlocalrule all:\n    input: ../results/fastqc/NA24631_1_fastqc.html, ../results/fastqc/NA24631_2_fastqc.html, ../results/fastqc/NA24631_1_fastqc.zip, ../results/fastqc/NA24631_2_fastqc.zip\n    jobid: 0\nresources: tmpdir=/dev/shm/jobs/26763281\n\n[Wed May 11 12:15:20 2022]\nFinished job 0.\n2 of 2 steps (100%) done\nComplete log: .snakemake/log/2022-05-11T121516.368334.snakemake.log\n</code></pre> <p></p> <p>We now have a log file, lets have a look at the first 10 lines of our log with:</p> <pre><code>head ./logs/fastqc/NA24631.log\n</code></pre> output <pre><code>Started analysis of NA24631_1.fastq.gz\nApprox 5% complete for NA24631_1.fastq.gz\nApprox 10% complete for NA24631_1.fastq.gz\nApprox 15% complete for NA24631_1.fastq.gz\nApprox 20% complete for NA24631_1.fastq.gz\nApprox 25% complete for NA24631_1.fastq.gz\nApprox 30% complete for NA24631_1.fastq.gz\nApprox 35% complete for NA24631_1.fastq.gz\nApprox 40% complete for NA24631_1.fastq.gz\nApprox 45% complete for NA24631_1.fastq.gz\n</code></pre> <p></p> <p>We have logs. Tidy logs.</p> <p></p> <p>Exercise:</p> <p>Try creating an error in the shell command (for example remove the <code>-o</code> flag) and use the three different syntaxes for writing to your log file. What is and isn't printed to your screen and to your log file?</p>"},{"location":"workshop_material/03_create_a_basic_workflow/#311-scale-up-to-analyse-all-of-our-samples","title":"3.11 Scale up to analyse all of our samples","text":"<p>We are currently only analysing one of our three samples</p> <p>Let's scale up to run all of our samples by using wildcards, this way we can grab all the samples/files in the <code>data</code> directory and analyse them</p> <ul> <li>Set a global wildcard that defines the samples to be analysed</li> <li>Generalise where this rule uses an individual sample (<code>NA24631</code>) to use this wildcard <code>{sample}</code></li> <li>Use the expand function (<code>expand()</code>) function to tell snakemake that <code>{sample}</code> is what we defined in our global wildcard <code>SAMPLES,</code></li> <li>Snakemake can figure out what <code>{sample}</code> is in our rule since it's defined in the targets in <code>rule all:</code></li> </ul> Edit snakefile <pre><code># define samples from data directory using wildcards\n+ SAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n# target OUTPUT files for the whole workflow\nrule all:\n    input:\n-       \"../results/fastqc/NA24631_1_fastqc.html\",\n-       \"../results/fastqc/NA24631_2_fastqc.html\",\n-       \"../results/fastqc/NA24631_1_fastqc.zip\",\n-       \"../results/fastqc/NA24631_2_fastqc.zip\"\n+       expand(\"../results/fastqc/{sample}_1_fastqc.html\", sample = SAMPLES),\n+       expand(\"../results/fastqc/{sample}_2_fastqc.html\", sample = SAMPLES),\n+       expand(\"../results/fastqc/{sample}_1_fastqc.zip\", sample = SAMPLES),\n+       expand(\"../results/fastqc/{sample}_2_fastqc.zip\", sample = SAMPLES)\n# workflow\nrule fastqc:\n    input:\n-       R1 = \"../../data/NA24631_1.fastq.gz\",\n-       R2 = \"../../data/NA24631_2.fastq.gz\"\n+       R1 = \"../../data/{sample}_1.fastq.gz\",\n+       R2 = \"../../data/{sample}_2.fastq.gz\"\n   output:\n-       html = [\"../results/fastqc/NA24631_1_fastqc.html\", \"../results/fastqc/NA24631_2_fastqc.html\"],\n-       zip = [\"../results/fastqc/NA24631_1_fastqc.zip\", \"../results/fastqc/NA24631_2_fastqc.zip\"]\n+       html = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\n+       zip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\n   log:\n-       \"logs/fastqc/NA24631.log\"\n+       \"logs/fastqc/{sample}.log\"\n   threads: 2\n    envmodules:\n        \"FastQC/0.11.9\"\n    shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} &amp;&gt; {log}\"\n</code></pre> Current snakefile: <pre><code># define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n# target OUTPUT files for the whole workflow\nrule all:\ninput:\nexpand(\"../results/fastqc/{sample}_1_fastqc.html\", sample = SAMPLES),\nexpand(\"../results/fastqc/{sample}_2_fastqc.html\", sample = SAMPLES),\nexpand(\"../results/fastqc/{sample}_1_fastqc.zip\", sample = SAMPLES),\nexpand(\"../results/fastqc/{sample}_2_fastqc.zip\", sample = SAMPLES)\n# workflow\nrule fastqc:\ninput:\nR1 = \"../../data/{sample}_1.fastq.gz\",\nR2 = \"../../data/{sample}_2.fastq.gz\"\noutput:\nhtml = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\nzip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\nlog:\n\"logs/fastqc/{sample}.log\"\nthreads: 2\nenvmodules:\n\"FastQC/0.11.9\"\nshell:\n\"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} &amp;&gt; {log}\"\n</code></pre> <p></p> <p>Visualise workflow</p> <pre><code>snakemake --dag | dot -Tpng &gt; dag_3.png\n</code></pre> <ul> <li>Now we have three samples running though our workflow, one of which has already been run in our last run (NA24631) indicated by the dashed lines</li> </ul> DAG <p></p> <p></p> <p>Run workflow again</p> <p><pre><code># remove output of last run\nrm -r ../results/*\n</code></pre> <pre><code># run dryrun again\nsnakemake --dryrun --cores 2 --use-envmodules\n</code></pre></p> <ul> <li>See how it now runs over all three of our samples in the output of the dryrun</li> </ul> output <pre><code>Building DAG of jobs...\nJob stats:\njob       count    min threads    max threads\n------  -------  -------------  -------------\nall           1              1              1\nfastqc        3              2              2\ntotal         4              1              2\n[Wed May 11 12:16:46 2022]\nrule fastqc:\n    input: ../../data/NA24695_1.fastq.gz, ../../data/NA24695_2.fastq.gz\n    output: ../results/fastqc/NA24695_1_fastqc.html, ../results/fastqc/NA24695_2_fastqc.html, ../results/fastqc/NA24695_1_fastqc.zip, ../results/fastqc/NA24695_2_fastqc.zip\n    log: logs/fastqc/NA24695.log\n    jobid: 2\nwildcards: sample=NA24695\n    threads: 2\nresources: tmpdir=/dev/shm/jobs/26763281\n\n[Wed May 11 12:16:46 2022]\nrule fastqc:\n    input: ../../data/NA24631_1.fastq.gz, ../../data/NA24631_2.fastq.gz\n    output: ../results/fastqc/NA24631_1_fastqc.html, ../results/fastqc/NA24631_2_fastqc.html, ../results/fastqc/NA24631_1_fastqc.zip, ../results/fastqc/NA24631_2_fastqc.zip\n    log: logs/fastqc/NA24631.log\n    jobid: 1\nwildcards: sample=NA24631\n    threads: 2\nresources: tmpdir=/dev/shm/jobs/26763281\n\n[Wed May 11 12:16:46 2022]\nrule fastqc:\n    input: ../../data/NA24694_1.fastq.gz, ../../data/NA24694_2.fastq.gz\n    output: ../results/fastqc/NA24694_1_fastqc.html, ../results/fastqc/NA24694_2_fastqc.html, ../results/fastqc/NA24694_1_fastqc.zip, ../results/fastqc/NA24694_2_fastqc.zip\n    log: logs/fastqc/NA24694.log\n    jobid: 3\nwildcards: sample=NA24694\n    threads: 2\nresources: tmpdir=/dev/shm/jobs/26763281\n\n[Wed May 11 12:16:46 2022]\nlocalrule all:\n    input: ../results/fastqc/NA24631_1_fastqc.html, ../results/fastqc/NA24695_1_fastqc.html, ../results/fastqc/NA24694_1_fastqc.html, ../results/fastqc/NA24631_2_fastqc.html, ../results/fastqc/NA24695_2_fastqc.html, ../results/fastqc/NA24694_2_fastqc.html, ../results/fastqc/NA24631_1_fastqc.zip, ../results/fastqc/NA24695_1_fastqc.zip, ../results/fastqc/NA24694_1_fastqc.zip, ../results/fastqc/NA24631_2_fastqc.zip, ../results/fastqc/NA24695_2_fastqc.zip, ../results/fastqc/NA24694_2_fastqc.zip\n    jobid: 0\nresources: tmpdir=/dev/shm/jobs/26763281\n\nJob stats:\njob       count    min threads    max threads\n------  -------  -------------  -------------\nall           1              1              1\nfastqc        3              2              2\ntotal         4              1              2\nThis was a dry-run (flag -n). The order of jobs does not reflect the order of execution.\n</code></pre> <p></p> <p>code</p> <pre><code># full run again\nsnakemake --cores 2 --use-envmodules\n</code></pre> <ul> <li>All three samples were run through our workflow! And we have a log file for each sample for the fastqc rule</li> </ul> <pre><code>ls -lh ./logs/fastqc\n</code></pre> output <pre><code>total 1.5K\n-rw-rw----+ 1 lkemp nesi99991 1.8K May 11 12:17 NA24631.log\n-rw-rw----+ 1 lkemp nesi99991 1.8K May 11 12:17 NA24694.log\n-rw-rw----+ 1 lkemp nesi99991 1.8K May 11 12:17 NA24695.log\n</code></pre> <p></p>"},{"location":"workshop_material/03_create_a_basic_workflow/#312-add-more-rules","title":"3.12 Add more rules","text":"<ul> <li>Connect the outputs of fastqc to the inputs of multiqc</li> <li>Add a new final target for <code>rule all:</code></li> </ul> Edit snakefile <pre><code># define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n\n# target OUTPUT files for the whole workflow\nrule all:\n    input:\n        expand(\"../results/fastqc/{sample}_1_fastqc.html\", sample = SAMPLES),\n        expand(\"../results/fastqc/{sample}_2_fastqc.html\", sample = SAMPLES),\n        expand(\"../results/fastqc/{sample}_1_fastqc.zip\", sample = SAMPLES),\n-       expand(\"../results/fastqc/{sample}_2_fastqc.zip\", sample = SAMPLES)\n+       expand(\"../results/fastqc/{sample}_2_fastqc.zip\", sample = SAMPLES),\n+       \"../results/multiqc_report.html\"\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/{sample}_1.fastq.gz\",\n        R2 = \"../../data/{sample}_2.fastq.gz\"\n    output:\n        html = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\n        zip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\n    log:\n        \"logs/fastqc/{sample}.log\"\n    threads: 2\n    envmodules:\n        \"FastQC/0.11.9\"\n    shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} &amp;&gt; {log}\"\n\n+ rule multiqc:\n+   input:\n+       expand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\n+   output:\n+       \"../results/multiqc_report.html\"\n+   log:\n+       \"logs/multiqc/multiqc.log\"\n+   envmodules:\n+       \"MultiQC/1.9-gimkl-2020a-Python-3.8.2\"\n+   shell:\n+       \"multiqc {input} -o ../results/ &amp;&gt; {log}\"\n</code></pre> Current snakefile: <pre><code># define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n# target OUTPUT files for the whole workflow\nrule all:\ninput:\nexpand(\"../results/fastqc/{sample}_1_fastqc.html\", sample = SAMPLES),\nexpand(\"../results/fastqc/{sample}_2_fastqc.html\", sample = SAMPLES),\nexpand(\"../results/fastqc/{sample}_1_fastqc.zip\", sample = SAMPLES),\nexpand(\"../results/fastqc/{sample}_2_fastqc.zip\", sample = SAMPLES),\n\"../results/multiqc_report.html\"\n# workflow\nrule fastqc:\ninput:\nR1 = \"../../data/{sample}_1.fastq.gz\",\nR2 = \"../../data/{sample}_2.fastq.gz\"\noutput:\nhtml = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\nzip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\nlog:\n\"logs/fastqc/{sample}.log\"\nthreads: 2\nenvmodules:\n\"FastQC/0.11.9\"\nshell:\n\"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} &amp;&gt; {log}\"\nrule multiqc:\ninput:\nexpand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\noutput:\n\"../results/multiqc_report.html\"\nlog:\n\"logs/multiqc/multiqc.log\"\nenvmodules:\n\"MultiQC/1.9-gimkl-2020a-Python-3.8.2\"\nshell:\n\"multiqc {input} -o ../results/ &amp;&gt; {log}\"\n</code></pre> <p>Run workflow again</p> <p><pre><code># remove output of last run\nrm -r ../results/*\n</code></pre> <pre><code># run dryrun/run again\nsnakemake --dryrun --cores 2 --use-envmodules\nsnakemake --cores 2 --use-envmodules\n</code></pre></p> <ul> <li> <p>Visualise workflow <pre><code>snakemake --dag | dot -Tpng &gt; dag_4.png\n</code></pre></p> </li> <li> <p>Now we have two rules in our workflow (fastqc and multiqc), we can also see that multiqc isn't run for each sample (since it merges the output of fastqc for all samples)</p> </li> </ul> DAG: <p></p> <p></p>"},{"location":"workshop_material/03_create_a_basic_workflow/#313-more-about-snakemakes-lazy-behaviour","title":"3.13 More about Snakemake's lazy behaviour","text":"<p>What happens if we only have the final target file (<code>../results/multiqc_report.html</code>) in <code>rule all:</code></p> Edit snakefile <pre><code># define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n\n# target OUTPUT files for the whole workflow\nrule all:\n    input:\n-       expand(\"../results/fastqc/{sample}_1_fastqc.html\", sample = SAMPLES),\n-       expand(\"../results/fastqc/{sample}_2_fastqc.html\", sample = SAMPLES),\n-       expand(\"../results/fastqc/{sample}_1_fastqc.zip\", sample = SAMPLES),\n-       expand(\"../results/fastqc/{sample}_2_fastqc.zip\", sample = SAMPLES),\n       \"../results/multiqc_report.html\"\n\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/{sample}_1.fastq.gz\",\n        R2 = \"../../data/{sample}_2.fastq.gz\"\n    output:\n        html = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\n        zip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\n    log:\n        \"logs/fastqc/{sample}.log\"\n    threads: 2\n    envmodules:\n        \"FastQC/0.11.9\"\n    shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} &amp;&gt; {log}\"\n\nrule multiqc:\n    input:\n        expand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\n    output:\n        \"../results/multiqc_report.html\"\n    log:\n        \"logs/multiqc/multiqc.log\"\n    envmodules:\n        \"MultiQC/1.9-gimkl-2020a-Python-3.8.2\"\n    shell:\n        \"multiqc {input} -o ../results/ &amp;&gt; {log}\"\n</code></pre> Current snakefile: <pre><code># define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n# target OUTPUT files for the whole workflow\nrule all:\ninput:\n\"../results/multiqc_report.html\"\n# workflow\nrule fastqc:\ninput:\nR1 = \"../../data/{sample}_1.fastq.gz\",\nR2 = \"../../data/{sample}_2.fastq.gz\"\noutput:\nhtml = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\nzip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\nlog:\n\"logs/fastqc/{sample}.log\"\nthreads: 2\nenvmodules:\n\"FastQC/0.11.9\"\nshell:\n\"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} &amp;&gt; {log}\"\nrule multiqc:\ninput:\nexpand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\noutput:\n\"../results/multiqc_report.html\"\nlog:\n\"logs/multiqc/multiqc.log\"\nenvmodules:\n\"MultiQC/1.9-gimkl-2020a-Python-3.8.2\"\nshell:\n\"multiqc {input} -o ../results/ &amp;&gt; {log}\"\n</code></pre> <p></p> <p>Run workflow again</p> <p><pre><code># remove output of last run\nrm -r ../results/*\n</code></pre> <pre><code># run dryrun again\nsnakemake --dryrun --cores 2 --use-envmodules\n</code></pre></p> <ul> <li> <p>It still works because it is the last file in the workflow sequence, Snakemake will do all the steps necessary to get to this target file (therefore it runs both fastqc and multiqc)</p> </li> <li> <p>Visualise workflow   <pre><code>snakemake --dag | dot -Tpng &gt; dag_5.png\n</code></pre></p> </li> <li> <p>Although the workflow ran the same, the DAG actually changed slightly, now there is only one file target and only the output of multiqc goes to <code>rule all</code></p> </li> </ul> DAG <p></p> <p></p> <p>Beware: Snakemake will also NOT run rules that it doesn't need to run in order to get the target files defined in rule: all</p> <p>For example if only our fastqc outputs are defined as the target in <code>rule: all</code></p> Edit snakefile <pre><code># define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n\n# target OUTPUT files for the whole workflow\nrule all:\n    input:\n+       expand(\"../results/fastqc/{sample}_1_fastqc.html\", sample = SAMPLES),\n+       expand(\"../results/fastqc/{sample}_2_fastqc.html\", sample = SAMPLES),\n+       expand(\"../results/fastqc/{sample}_1_fastqc.zip\", sample = SAMPLES),\n+       expand(\"../results/fastqc/{sample}_2_fastqc.zip\", sample = SAMPLES)\n-       \"../results/multiqc_report.html\"\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/{sample}_1.fastq.gz\",\n        R2 = \"../../data/{sample}_2.fastq.gz\"\n    output:\n        html = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\n        zip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\n    log:\n        \"logs/fastqc/{sample}.log\"\n    threads: 2\n    envmodules:\n        \"FastQC/0.11.9\"\n    shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} &amp;&gt; {log}\"\n\nrule multiqc:\n    input:\n        expand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\n    output:\n        \"../results/multiqc_report.html\"\n    log:\n        \"logs/multiqc/multiqc.log\"\n    envmodules:\n        \"MultiQC/1.9-gimkl-2020a-Python-3.8.2\"\n    shell:\n        \"multiqc {input} -o ../results/ &amp;&gt; {log}\"\n</code></pre> Current snakefile: <pre><code># define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n# target OUTPUT files for the whole workflow\nrule all:\ninput:\nexpand(\"../results/fastqc/{sample}_1_fastqc.html\", sample = SAMPLES),\nexpand(\"../results/fastqc/{sample}_2_fastqc.html\", sample = SAMPLES),\nexpand(\"../results/fastqc/{sample}_1_fastqc.zip\", sample = SAMPLES),\nexpand(\"../results/fastqc/{sample}_2_fastqc.zip\", sample = SAMPLES)\n# workflow\nrule fastqc:\ninput:\nR1 = \"../../data/{sample}_1.fastq.gz\",\nR2 = \"../../data/{sample}_2.fastq.gz\"\noutput:\nhtml = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\nzip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\nlog:\n\"logs/fastqc/{sample}.log\"\nthreads: 2\nenvmodules:\n\"FastQC/0.11.9\"\nshell:\n\"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} &amp;&gt; {log}\"\nrule multiqc:\ninput:\nexpand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\noutput:\n\"../results/multiqc_report.html\"\nlog:\n\"logs/multiqc/multiqc.log\"\nenvmodules:\n\"MultiQC/1.9-gimkl-2020a-Python-3.8.2\"\nshell:\n\"multiqc {input} -o ../results/ &amp;&gt; {log}\"\n</code></pre> <p></p> <p>Run again</p> <pre><code># run dryrun again\nsnakemake --dryrun --cores 2 --use-envmodules\n</code></pre> <p>My partial output:</p> <pre><code>Job stats:\njob       count    min threads    max threads\n------  -------  -------------  -------------\nall           1              1              1\nfastqc        3              2              2\ntotal         4              1              2\n</code></pre> <p></p> <p>Our multiqc rule won't be run/evaluated : Visualise workflow</p> <pre><code>snakemake --dag | dot -Tpng &gt; dag_6.png\n</code></pre> <ul> <li>Now we are back to only running fastqc in our workflow, despite having our second rule (multiqc) in our workflow</li> </ul> DAG: <p></p> <p></p> <p>Snakemake is lazy.</p> <p></p>"},{"location":"workshop_material/03_create_a_basic_workflow/#314-add-even-more-rules","title":"3.14 Add even more rules","text":"<p>Let's add the rest of the rules. We want to get to:</p> <p></p> <p>We currently have fastqc and multiqc, so we still need to add trim_galore</p> Edit snakefile <pre><code># define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n\n# target OUTPUT files for the whole workflow\nrule all:\n    input:\n-       expand(\"../results/fastqc/{sample}_1_fastqc.html\", sample = SAMPLES),\n-       expand(\"../results/fastqc/{sample}_2_fastqc.html\", sample = SAMPLES),\n-       expand(\"../results/fastqc/{sample}_1_fastqc.zip\", sample = SAMPLES),\n-       expand(\"../results/fastqc/{sample}_2_fastqc.zip\", sample = SAMPLES)\n+       \"../results/multiqc_report.html\",\n+       expand([\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"], sample = SAMPLES)\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/{sample}_1.fastq.gz\",\n        R2 = \"../../data/{sample}_2.fastq.gz\"\n    output:\n        html = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\n        zip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\n    log:\n        \"logs/fastqc/{sample}.log\"\n    threads: 2\n    envmodules:\n        \"FastQC/0.11.9\"\n    shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} &amp;&gt; {log}\"\n\nrule multiqc:\n    input:\n        expand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\n    output:\n        \"../results/multiqc_report.html\"\n    log:\n        \"logs/multiqc/multiqc.log\"\n    envmodules:\n        \"MultiQC/1.9-gimkl-2020a-Python-3.8.2\"\n    shell:\n        \"multiqc {input} -o ../results/ &amp;&gt; {log}\"\n\n+ rule trim_galore:\n+   input:\n+       [\"../../data/{sample}_1.fastq.gz\", \"../../data/{sample}_2.fastq.gz\"]\n+   output:\n+       [\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"]\n+   log:\n+       \"logs/trim_galore/{sample}.log\"\n+   envmodules:\n+       \"TrimGalore/0.6.7-gimkl-2020a-Python-3.8.2-Perl-5.30.1\"\n+   threads: 2\n+   shell:\n+       \"trim_galore {input} -o ../results/trimmed/ --paired --cores {threads} &amp;&gt; {log}\"\n</code></pre> Current snakefile: <pre><code># define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n# target OUTPUT files for the whole workflow\nrule all:\ninput:\n\"../results/multiqc_report.html\",\nexpand([\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"], sample = SAMPLES)\n# workflow\nrule fastqc:\ninput:\nR1 = \"../../data/{sample}_1.fastq.gz\",\nR2 = \"../../data/{sample}_2.fastq.gz\"\noutput:\nhtml = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\nzip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\nlog:\n\"logs/fastqc/{sample}.log\"\nthreads: 2\nenvmodules:\n\"FastQC/0.11.9\"\nshell:\n\"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} &amp;&gt; {log}\"\nrule multiqc:\ninput:\nexpand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\noutput:\n\"../results/multiqc_report.html\"\nlog:\n\"logs/multiqc/multiqc.log\"\nenvmodules:\n\"MultiQC/1.9-gimkl-2020a-Python-3.8.2\"\nshell:\n\"multiqc {input} -o ../results/ &amp;&gt; {log}\"\nrule trim_galore:\ninput:\n[\"../../data/{sample}_1.fastq.gz\", \"../../data/{sample}_2.fastq.gz\"]\noutput:\n[\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"]\nlog:\n\"logs/trim_galore/{sample}.log\"\nenvmodules:\n\"TrimGalore/0.6.7-gimkl-2020a-Python-3.8.2-Perl-5.30.1\"\nthreads: 2\nshell:\n\"trim_galore {input} -o ../results/trimmed/ --paired --cores {threads} &amp;&gt; {log}\"\n</code></pre> <p></p> <p>Visualise workflow</p> <pre><code>snakemake --dag | dot -Tpng &gt; dag_7.png\n</code></pre> <p>Fantastic, we are starting to build a workflow!</p> DAG: <p></p> <p></p> <p>However, when analysing many samples, our DAG can become messy and complicated. Instead, we can create a rulegraph that will let us visualise our workflow without showing every single sample that will run through it</p> <p>code</p> <pre><code>snakemake --rulegraph | dot -Tpng &gt; rulegraph_1.png\n</code></pre> <p>My rulegraph:</p> <p></p> <p></p> <p>An aside: another option that will show all your input and output files at each step:</p> <pre><code>snakemake --filegraph | dot -Tpng &gt; filegraph.png\n</code></pre> My filegraph: <p></p> <p></p> <p>Run the rest of the workflow</p> <p>code</p> <pre><code># run dryrun/run again\nsnakemake --dryrun --cores 2 --use-envmodules\nsnakemake --cores 2 --use-envmodules\n</code></pre> <p>Notice it will run only one rule/sample/file at a time...why is that?</p>"},{"location":"workshop_material/03_create_a_basic_workflow/#315-throw-it-more-cores","title":"3.15 Throw it more cores","text":"<p>Run again allowing Snakemake to use more cores overall <code>--cores 4</code> rather than <code>--cores 2</code></p> <p>code</p> <p><pre><code># remove output of last run\nrm -r ../results/*\n</code></pre> <pre><code># run dryrun/run again\nsnakemake --dryrun --cores 4 --use-envmodules\nsnakemake --cores 4 --use-envmodules\n</code></pre></p> <p>Notice the whole workflow ran much faster and several samples/files/rules were running at one time. This is because we set each rule to run with 2 threads. Initially we specified that the maximum number of cores to be used by the workflow was 2 with the <code>--cores 2</code> flag, meaning only one rule and sample can be run at one time. When we increased the maximum number of cores to be used by the workflow to 4 with <code>--cores 4</code>, up to 2 samples could be run through at one time.</p>"},{"location":"workshop_material/03_create_a_basic_workflow/#316-throw-it-even-more-cores","title":"3.16 Throw it even more cores","text":"<p>With a high performance cluster such as NeSi, you can start to REALLY scale up, particularly when you have many samples to analyse or files to process. This is because the number of cores available in a HPC is HUGE compared to a laptop or even an high end server.</p> <p>Boom! Scalability here we come!</p> <p></p> <p>To run the workflow on the cluster, we need to ensure that each step is run as a dedicated job in the queuing system of the HPC. On NeSI, the queuing system is managed by Slurm.</p> <p>Use the <code>--cluster</code> option to specify the job submission command, using <code>sbatch</code> on NeSI. This command defines resources used for each job (maximum time, memory, number of cores...). In addition, you need to specify a maximum number of concurrent jobs using <code>--jobs</code>.</p> <p>code</p> <p><pre><code># remove output of last run\nrm -r ../results/*\n</code></pre> <pre><code># run again on the cluster\nsnakemake --cluster \"sbatch --time 00:10:00 --mem 512MB --cpus-per-task 8 --account nesi99991\" --jobs 10 --use-envmodules\n</code></pre></p> output <pre><code>Building DAG of jobs...\nUsing shell: /usr/bin/bash\nProvided cores: 4\nRules claiming more threads will be scaled down.\nJob stats:\njob            count    min threads    max threads\n-----------  -------  -------------  -------------\nall                1              1              1\nfastqc             3              2              2\nmultiqc            1              1              1\ntrim_galore        3              2              2\ntotal              8              1              2\nSelect jobs to execute...\n\n[Wed May 11 12:26:39 2022]\nrule fastqc:\n    input: ../../data/NA24694_1.fastq.gz, ../../data/NA24694_2.fastq.gz\n    output: ../results/fastqc/NA24694_1_fastqc.html, ../results/fastqc/NA24694_2_fastqc.html, ../results/fastqc/NA24694_1_fastqc.zip, ../results/fastqc/NA24694_2_fastqc.zip\n    log: logs/fastqc/NA24694.log\n    jobid: 4\nwildcards: sample=NA24694\n    threads: 2\nresources: tmpdir=/dev/shm/jobs/26763281\n\nActivating environment modules: FastQC/0.11.9\n\n[Wed May 11 12:26:39 2022]\nrule trim_galore:\n    input: ../../data/NA24694_1.fastq.gz, ../../data/NA24694_2.fastq.gz\n    output: ../results/trimmed/NA24694_1_val_1.fq.gz, ../results/trimmed/NA24694_2_val_2.fq.gz\n    log: logs/trim_galore/NA24694.log\n    jobid: 7\nwildcards: sample=NA24694\n    threads: 2\nresources: tmpdir=/dev/shm/jobs/26763281\n\nActivating environment modules: TrimGalore/0.6.7-gimkl-2020a-Python-3.8.2-Perl-5.30.1\n\nThe following modules were not unloaded:\n   (Use \"module --force purge\" to unload all):\n\n1) XALT/minimal   2) slurm   3) NeSI\n\nThe following modules were not unloaded:\n   (Use \"module --force purge\" to unload all):\n\n1) XALT/minimal   2) slurm   3) NeSI\n[Wed May 11 12:26:44 2022]\nFinished job 4.\n1 of 8 steps (12%) done\nSelect jobs to execute...\n\n[Wed May 11 12:26:44 2022]\nrule fastqc:\n    input: ../../data/NA24631_1.fastq.gz, ../../data/NA24631_2.fastq.gz\n    output: ../results/fastqc/NA24631_1_fastqc.html, ../results/fastqc/NA24631_2_fastqc.html, ../results/fastqc/NA24631_1_fastqc.zip, ../results/fastqc/NA24631_2_fastqc.zip\n    log: logs/fastqc/NA24631.log\n    jobid: 2\nwildcards: sample=NA24631\n    threads: 2\nresources: tmpdir=/dev/shm/jobs/26763281\n\nActivating environment modules: FastQC/0.11.9\n\nThe following modules were not unloaded:\n   (Use \"module --force purge\" to unload all):\n\n1) XALT/minimal   2) slurm   3) NeSI\n[Wed May 11 12:26:47 2022]\nFinished job 7.\n2 of 8 steps (25%) done\nSelect jobs to execute...\n\n[Wed May 11 12:26:47 2022]\nrule trim_galore:\n    input: ../../data/NA24631_1.fastq.gz, ../../data/NA24631_2.fastq.gz\n    output: ../results/trimmed/NA24631_1_val_1.fq.gz, ../results/trimmed/NA24631_2_val_2.fq.gz\n    log: logs/trim_galore/NA24631.log\n    jobid: 5\nwildcards: sample=NA24631\n    threads: 2\nresources: tmpdir=/dev/shm/jobs/26763281\n\nActivating environment modules: TrimGalore/0.6.7-gimkl-2020a-Python-3.8.2-Perl-5.30.1\n\nThe following modules were not unloaded:\n   (Use \"module --force purge\" to unload all):\n\n1) XALT/minimal   2) slurm   3) NeSI\n[Wed May 11 12:26:50 2022]\nFinished job 2.\n3 of 8 steps (38%) done\nSelect jobs to execute...\n\n[Wed May 11 12:26:50 2022]\nrule fastqc:\n    input: ../../data/NA24695_1.fastq.gz, ../../data/NA24695_2.fastq.gz\n    output: ../results/fastqc/NA24695_1_fastqc.html, ../results/fastqc/NA24695_2_fastqc.html, ../results/fastqc/NA24695_1_fastqc.zip, ../results/fastqc/NA24695_2_fastqc.zip\n    log: logs/fastqc/NA24695.log\n    jobid: 3\nwildcards: sample=NA24695\n    threads: 2\nresources: tmpdir=/dev/shm/jobs/26763281\n\nActivating environment modules: FastQC/0.11.9\n\nThe following modules were not unloaded:\n   (Use \"module --force purge\" to unload all):\n\n1) XALT/minimal   2) slurm   3) NeSI\n[Wed May 11 12:26:54 2022]\nFinished job 3.\n4 of 8 steps (50%) done\nSelect jobs to execute...\n\n[Wed May 11 12:26:54 2022]\nrule trim_galore:\n    input: ../../data/NA24695_1.fastq.gz, ../../data/NA24695_2.fastq.gz\n    output: ../results/trimmed/NA24695_1_val_1.fq.gz, ../results/trimmed/NA24695_2_val_2.fq.gz\n    log: logs/trim_galore/NA24695.log\n    jobid: 6\nwildcards: sample=NA24695\n    threads: 2\nresources: tmpdir=/dev/shm/jobs/26763281\n\nActivating environment modules: TrimGalore/0.6.7-gimkl-2020a-Python-3.8.2-Perl-5.30.1\n\nThe following modules were not unloaded:\n   (Use \"module --force purge\" to unload all):\n\n1) XALT/minimal   2) slurm   3) NeSI\n[Wed May 11 12:26:56 2022]\nFinished job 5.\n5 of 8 steps (62%) done\nSelect jobs to execute...\n\n[Wed May 11 12:26:56 2022]\nrule multiqc:\n    input: ../results/fastqc/NA24631_1_fastqc.zip, ../results/fastqc/NA24695_1_fastqc.zip, ../results/fastqc/NA24694_1_fastqc.zip, ../results/fastqc/NA24631_2_fastqc.zip, ../results/fastqc/NA24695_2_fastqc.zip, ../results/fastqc/NA24694_2_fastqc.zip\n    output: ../results/multiqc_report.html\n    log: logs/multiqc/multiqc.log\n    jobid: 1\nresources: tmpdir=/dev/shm/jobs/26763281\n\nActivating environment modules: MultiQC/1.9-gimkl-2020a-Python-3.8.2\n\nThe following modules were not unloaded:\n   (Use \"module --force purge\" to unload all):\n\n1) XALT/minimal   2) slurm   3) NeSI\n[Wed May 11 12:27:01 2022]\nFinished job 6.\n6 of 8 steps (75%) done\n[Wed May 11 12:27:03 2022]\nFinished job 1.\n7 of 8 steps (88%) done\nSelect jobs to execute...\n\n[Wed May 11 12:27:03 2022]\nlocalrule all:\n    input: ../results/multiqc_report.html, ../results/trimmed/NA24631_1_val_1.fq.gz, ../results/trimmed/NA24695_1_val_1.fq.gz, ../results/trimmed/NA24694_1_val_1.fq.gz, ../results/trimmed/NA24631_2_val_2.fq.gz, ../results/trimmed/NA24695_2_val_2.fq.gz, ../results/trimmed/NA24694_2_val_2.fq.gz\n    jobid: 0\nresources: tmpdir=/dev/shm/jobs/26763281\n\n[Wed May 11 12:27:03 2022]\nFinished job 0.\n8 of 8 steps (100%) done\nComplete log: .snakemake/log/2022-05-11T122639.019945.snakemake.log\n</code></pre> <p></p> <p>If you open another terminal on the HPC, you can use the <code>squeue</code> command to list of your jobs and their state (pending, running, etc.):</p> <p>code</p> <pre><code>squeue --me\n</code></pre> output <pre><code>JOBID         USER     ACCOUNT   NAME        CPUS MIN_MEM PARTITI START_TIME     TIME_LEFT STATE    NODELIST(REASON)    26763281      lkemp    nesi99991 spawner-jupy   4      4G interac 2022-05-11T1     7:30:33 RUNNING  wbn003              26763418      lkemp    nesi99991 snakejob.fas   8    512M large   2022-05-11T1        9:59 RUNNING  wbn096              26763419      lkemp    nesi99991 snakejob.tri   8    512M large   2022-05-11T1        9:59 RUNNING  wbn096              26763420      lkemp    nesi99991 snakejob.fas   8    512M large   2022-05-11T1        9:59 RUNNING  wbn110              26763421      lkemp    nesi99991 snakejob.fas   8    512M large   2022-05-11T1        9:59 RUNNING  wbn069              26763422      lkemp    nesi99991 snakejob.tri   8    512M large   2022-05-11T1        9:59 RUNNING  wbn070              26763423      lkemp    nesi99991 snakejob.tri   8    512M large   2022-05-11T1        9:59 RUNNING  wbn090  </code></pre> <p></p> <p>An additional trick is to use the <code>watch</code> command to repeatedly call any command in the terminal, giving you a lightweight monitoring tool ;-). Here we will use it to see your jobs gets queued and executed in real time:</p> <p>code</p> <pre><code>watch squeue --me\n</code></pre> <p>You can exit the view create by <code>watch</code> by pressing CTRL+C.</p>"},{"location":"workshop_material/03_create_a_basic_workflow/#takeaways","title":"Takeaways","text":"<ul> <li>Once familiar with environment modules, the software are very straightforward to integrate in your snakemake workflow</li> <li>Run your commands directly on the command line before wrapping it up in a Snakemake rule</li> <li>First do a dryrun to check the Snakemake structure is set up correctly</li> <li>Work iteratively (get each rule working before moving onto the next)</li> <li>File paths are relative to the Snakefile</li> <li>Run your workflow from where your Snakefile is</li> <li>Visualise your workflow by creating a DAG (directed acyclic graph), a rulegraph or filegraph</li> <li>Use environment modules to load software in your workflow - this improves reproducibility</li> <li>Snakemake is lazy...</li> <li>It will only do something if it hasn't already done it</li> <li>It will pick up where it left off, rather than run the whole workflow again</li> <li>It won't do any steps that aren't necessary to get to the target files defined in <code>rule: all</code></li> <li><code>input:</code> <code>output:</code> <code>log:</code> and <code>threads:</code> directives need to be called in the <code>shell</code> directive</li> <li>Capture your log files</li> <li>Organise your log files by naming them after the rule that was run and sample that was analysed</li> <li>You don't need to specify all the target files in <code>rule all:</code>, the final file in a given chain of tasks will suffice</li> <li>We can massively speed up our analyses by running our samples in parallel</li> </ul>"},{"location":"workshop_material/03_create_a_basic_workflow/#summary-commands","title":"Summary commands","text":"<p>code</p> <ul> <li>Create a directed acyclic graph (DAG) with:</li> </ul> <pre><code>snakemake --dag | dot -Tpng &gt; dag.png\n</code></pre> <ul> <li>Create a rulegraph with:</li> </ul> <pre><code>snakemake --rulegraph | dot -Tpng &gt; rulegraph.png\n</code></pre> <ul> <li>Create a filegraph with:</li> </ul> <pre><code>snakemake --filegraph | dot -Tpng &gt; filegraph.png\n</code></pre> <ul> <li>Run a dryrun of your snakemake workflow with:</li> </ul> <pre><code>snakemake --dryrun\n</code></pre> <ul> <li>Run your snakemake workflow with:</li> </ul> <pre><code>snakemake --cores 2\n</code></pre> <ul> <li>Run a dryrun of your snakemake workflow (using environment modules to load your software) with:</li> </ul> <pre><code>snakemake --dryrun --cores 2 --use-envmodules\n</code></pre> <ul> <li>Run your snakemake workflow (using environment modules to load your software) with:</li> </ul> <pre><code>snakemake --cores 2 --use-envmodules\n</code></pre> <ul> <li>Run your snakemake workflow using multiple jobs on NeSI:</li> </ul> <pre><code>snakemake --cluster \"sbatch --time 00:10:00 --mem=512MB --cpus-per-task 8\" --jobs 10 --use-envmodules\n</code></pre> <ul> <li>Create a global wildcard to get process all your samples in a directory with:</li> </ul> <pre><code>SAMPLES, = glob_wildcards(\"../relative/path/to/samples/{sample}_1.fastq.gz\")\n</code></pre> <ul> <li>Combine this with the expand function to tell Snakemake to look at your global wildcard to figure out what you refer to as <code>{sample}</code> in your workflow</li> </ul> <pre><code>expand(\"../results/{sample}_1.fastq.gz\", sample = SAMPLES)\n</code></pre> <ul> <li>Increase the number of samples that can be analysed at one time in your workflow by increasing the maximum number of cores to be used at one time with the <code>--cores</code> command</li> </ul> <pre><code>snakemake --cores 4 --use-envmodules\n</code></pre>"},{"location":"workshop_material/03_create_a_basic_workflow/#our-final-snakemake-workflow","title":"Our final snakemake workflow!","text":"<p>See basic_demo_workflow for the final Snakemake workflow we've created up to this point</p> <p>Back to homepage</p>"},{"location":"workshop_material/04_leveling_up_your_workflow/","title":"04 - Leveling up your workflow!","text":""},{"location":"workshop_material/04_leveling_up_your_workflow/#catching-up","title":"Catching up","text":"<p>From section 03, you should have the following Snakefile:</p> <pre><code># define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n# target OUTPUT files for the whole workflow\nrule all:\ninput:\n\"../results/multiqc_report.html\",\nexpand([\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"], sample = SAMPLES)\n# workflow\nrule fastqc:\ninput:\nR1 = \"../../data/{sample}_1.fastq.gz\",\nR2 = \"../../data/{sample}_2.fastq.gz\"\noutput:\nhtml = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\nzip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\nlog:\n\"logs/fastqc/{sample}.log\"\nthreads: 2\nenvmodules:\n\"FastQC/0.11.9\"\nshell:\n\"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} &amp;&gt; {log}\"\nrule multiqc:\ninput:\nexpand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\noutput:\n\"../results/multiqc_report.html\"\nlog:\n\"logs/multiqc/multiqc.log\"\nenvmodules:\n\"MultiQC/1.9-gimkl-2020a-Python-3.8.2\"\nshell:\n\"multiqc {input} -o ../results/ &amp;&gt; {log}\"\nrule trim_galore:\ninput:\n[\"../../data/{sample}_1.fastq.gz\", \"../../data/{sample}_2.fastq.gz\"]\noutput:\n[\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"]\nlog:\n\"logs/trim_galore/{sample}.log\"\nenvmodules:\n\"TrimGalore/0.6.7-gimkl-2020a-Python-3.8.2-Perl-5.30.1\"\nthreads: 2\nshell:\n\"trim_galore {input} -o ../results/trimmed/ --paired --cores {threads} &amp;&gt; {log}\"\n</code></pre> <p></p>"},{"location":"workshop_material/04_leveling_up_your_workflow/#41-use-a-profile-for-hpc","title":"4.1 Use a profile for HPC","text":"<p>In section 3.16, we have seen that a snakemake workflow can be run on an HPC cluster. To reduce the boilerplate, we can use a configuration profile to configure default options. In this case, we use it to set the <code>--cluster</code> and the <code>--jobs</code> options.</p> <p>Make a <code>slurm</code> profile folder</p> <pre><code># create the profile folder\nmkdir slurm\ntouch slurm/config.yaml\n</code></pre> <p>write the following to config.yaml</p> <pre><code>jobs: 20\ncluster: \"sbatch --time 00:10:00 --mem 512MB --cpus-per-task 8 --account nesi99991\"\n</code></pre> <p>Then run the snakemake workflow using the <code>slurm</code> profile</p> <p><pre><code># remove output of last run\nrm -r ../results/*\n</code></pre> <pre><code># run dryrun/run again\nsnakemake --dryrun --profile slurm --use-envmodules\n</code></pre> <pre><code>snakemake --profile slurm --use-envmodules\n</code></pre></p> <p>If you interrupt the execution of a snakemake workflow using CTRL+C, already submitted Slurm jobs won't be cancelled. We tell snakemake how to cancel Slurm jobs using <code>scancel</code> via the <code>--cluster-cancel</code> option and adding <code>--parsable</code> to the <code>sbatch</code> command, to make it return the job ID.</p> <p>Edit config.yaml</p> <pre><code>jobs: 20\n- cluster: \"sbatch --time 00:10:00 --mem 512MB --cpus-per-task 8\"\n+ cluster: \"sbatch --parsable --time 00:10:00 --mem 512MB --cpus-per-task 8\"\n+ cluster-cancel: scancel\n</code></pre> <p>You can specify different resources (memory, cpus, gpus, etc.) for each target in the workflow and refer to them in the <code>cluster</code> option using placeholders. Default resources for all rules can also be set using the <code>default-resources</code> option.</p> <p>Update the profile <code>slurm/config.yaml</code> file as follows (using a multiline option to improve readability)</p> <p>Edit config.yml</p> <pre><code>jobs: 20\n- cluster: \"sbatch --parsable --time 00:10:00 --mem 512MB --cpus-per-task 8\"\n+ cluster:\n+     sbatch\n+         --parsable\n+         --time {resources.time_min}\n+         --mem {resources.mem_mb}\n+         --cpus-per-task {resources.cpus}\n+         --account nesi99991\n+ default-resources: [cpus=2, mem_mb=512, time_min=10]\ncluster-cancel: scancel\n</code></pre> <p>and add resources definitions in the workflow. Here we give more CPU resources to <code>trim_galore</code> to make it run faster.</p> Edit snakefile <pre><code># define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n\n# target OUTPUT files for the whole workflow\nrule all:\n    input:\n        \"../results/multiqc_report.html\",\n        expand([\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"], sample = SAMPLES)\n\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/{sample}_1.fastq.gz\",\n        R2 = \"../../data/{sample}_2.fastq.gz\"\n    output:\n        html = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\n        zip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\n    log:\n        \"logs/fastqc/{sample}.log\"\n    threads: 2\n    envmodules:\n        \"FastQC/0.11.9\"\n    shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} &amp;&gt; {log}\"\n\nrule multiqc:\n    input:\n        expand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\n    output:\n        \"../results/multiqc_report.html\"\n    log:\n        \"logs/multiqc/multiqc.log\"\n    envmodules:\n        \"MultiQC/1.9-gimkl-2020a-Python-3.8.2\"\n    shell:\n        \"multiqc {input} -o ../results/ &amp;&gt; {log}\"\n\nrule trim_galore:\n    input:\n        [\"../../data/{sample}_1.fastq.gz\", \"../../data/{sample}_2.fastq.gz\"]\n    output:\n        [\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"]\n    log:\n        \"logs/trim_galore/{sample}.log\"\n    envmodules:\n        \"TrimGalore/0.6.7-gimkl-2020a-Python-3.8.2-Perl-5.30.1\"\n    threads: 2\n+   resources:\n+       cpus=8\n   shell:\n        \"trim_galore {input} -o ../results/trimmed/ --paired --cores {threads} &amp;&gt; {log}\"\n</code></pre> Current slurm profile: <pre><code>jobs: 20\ncluster:\n    sbatch\n        --parsable\n        --time {resources.time_min}\n        --mem {resources.mem_mb}\n        --cpus-per-task {resources.cpus}\n        --account nesi99991\ndefault-resources: [cpus=2, mem_mb=512, time_min=10]\ncluster-cancel: scancel\n</code></pre> Current snakefile: <pre><code># define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n# target OUTPUT files for the whole workflow\nrule all:\ninput:\n\"../results/multiqc_report.html\",\nexpand([\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"], sample = SAMPLES)\n# workflow\nrule fastqc:\ninput:\nR1 = \"../../data/{sample}_1.fastq.gz\",\nR2 = \"../../data/{sample}_2.fastq.gz\"\noutput:\nhtml = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\nzip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\nlog:\n\"logs/fastqc/{sample}.log\"\nthreads: 2\nenvmodules:\n\"FastQC/0.11.9\"\nshell:\n\"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} &amp;&gt; {log}\"\nrule multiqc:\ninput:\nexpand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\noutput:\n\"../results/multiqc_report.html\"\nlog:\n\"logs/multiqc/multiqc.log\"\nenvmodules:\n\"MultiQC/1.9-gimkl-2020a-Python-3.8.2\"\nshell:\n\"multiqc {input} -o ../results/ &amp;&gt; {log}\"\nrule trim_galore:\ninput:\n[\"../../data/{sample}_1.fastq.gz\", \"../../data/{sample}_2.fastq.gz\"]\noutput:\n[\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"]\nlog:\n\"logs/trim_galore/{sample}.log\"\nenvmodules:\n\"TrimGalore/0.6.7-gimkl-2020a-Python-3.8.2-Perl-5.30.1\"\nthreads: 2\nresources:\ncpus=8\nshell:\n\"trim_galore {input} -o ../results/trimmed/ --paired --cores {threads} &amp;&gt; {log}\"\n</code></pre> <p></p> <p>Run the workflow again</p> <p><pre><code># remove output of last run\nrm -r ../results/*\n</code></pre> <pre><code># run dryrun/run again\nsnakemake --dryrun --profile slurm --use-envmodules\n</code></pre> <pre><code>snakemake --profile slurm --use-envmodules\n</code></pre></p> <ul> <li>If you monitor the progress of your jobs using <code>squeue</code>, you will notice that some jobs now request 2 or 8 CPUs.</li> </ul> output <pre><code>JOBID         USER     ACCOUNT   NAME        CPUS MIN_MEM PARTITI START_TIME     TIME_LEFT STATE    NODELIST(REASON)\n26763281      lkemp    nesi99991 spawner-jupy   4      4G interac 2022-05-11T1     7:21:18 RUNNING  wbn003\n26763492      lkemp    nesi99991 snakejob.fas   2    512M large   2022-05-11T1        9:59 RUNNING  wbn144\n26763493      lkemp    nesi99991 snakejob.tri   8    512M large   2022-05-11T1        9:59 RUNNING  wbn212\n26763494      lkemp    nesi99991 snakejob.fas   2    512M large   2022-05-11T1        9:59 RUNNING  wbn145\n26763495      lkemp    nesi99991 snakejob.fas   2    512M large   2022-05-11T1        9:59 RUNNING  wbn146\n26763496      lkemp    nesi99991 snakejob.tri   8    512M large   2022-05-11T1        9:59 RUNNING  wbn217\n26763497      lkemp    nesi99991 snakejob.tri   8    512M large   2022-05-11T1        9:59 RUNNING  wbn229\n</code></pre> <p></p> <p>Now looking at the content of our workflow folder, it is getting cluttered with Slurm log files:</p> <p>code</p> <pre><code>ls -lh\n</code></pre> output <pre><code>total 1.8M\n-rw-rw----+ 1 lkemp nesi99991 4.2K May 11 12:10 dag_1.png\n-rw-rw----+ 1 lkemp nesi99991 3.8K May 11 12:13 dag_2.png\n-rw-rw----+ 1 lkemp nesi99991  12K May 11 12:16 dag_3.png\n-rw-rw----+ 1 lkemp nesi99991  20K May 11 12:19 dag_4.png\n-rw-rw----+ 1 lkemp nesi99991  15K May 11 12:21 dag_5.png\n-rw-rw----+ 1 lkemp nesi99991  12K May 11 12:23 dag_6.png\n-rw-rw----+ 1 lkemp nesi99991  26K May 11 12:24 dag_7.png\ndrwxrws---+ 5 lkemp nesi99991 4.0K May 11 12:25 logs\n-rw-rw----+ 1 lkemp nesi99991  11K May 11 12:24 rulegraph_1.png\ndrwxrws---+ 3 lkemp nesi99991 4.0K May 11 12:34 slurm\n-rw-rw----+ 1 lkemp nesi99991  837 May 11 12:27 slurm-26763403.out\n-rw-rw----+ 1 lkemp nesi99991  809 May 11 12:27 slurm-26763404.out\n-rw-rw----+ 1 lkemp nesi99991  837 May 11 12:27 slurm-26763405.out\n-rw-rw----+ 1 lkemp nesi99991  837 May 11 12:27 slurm-26763406.out\n-rw-rw----+ 1 lkemp nesi99991  809 May 11 12:27 slurm-26763407.out\n-rw-rw----+ 1 lkemp nesi99991  809 May 11 12:27 slurm-26763408.out\n-rw-rw----+ 1 lkemp nesi99991  865 May 11 12:29 slurm-26763409.out\n-rw-rw----+ 1 lkemp nesi99991  837 May 11 12:30 slurm-26763418.out\n-rw-rw----+ 1 lkemp nesi99991  809 May 11 12:30 slurm-26763419.out\n-rw-rw----+ 1 lkemp nesi99991  837 May 11 12:30 slurm-26763420.out\n-rw-rw----+ 1 lkemp nesi99991  837 May 11 12:30 slurm-26763421.out\n-rw-rw----+ 1 lkemp nesi99991  809 May 11 12:30 slurm-26763422.out\n-rw-rw----+ 1 lkemp nesi99991  809 May 11 12:30 slurm-26763423.out\n-rw-rw----+ 1 lkemp nesi99991  865 May 11 12:32 slurm-26763431.out\n-rw-rw----+ 1 lkemp nesi99991  837 May 11 12:33 slurm-26763435.out\n-rw-rw----+ 1 lkemp nesi99991  809 May 11 12:34 slurm-26763436.out\n-rw-rw----+ 1 lkemp nesi99991  837 May 11 12:33 slurm-26763437.out\n-rw-rw----+ 1 lkemp nesi99991  837 May 11 12:34 slurm-26763438.out\n-rw-rw----+ 1 lkemp nesi99991  809 May 11 12:34 slurm-26763439.out\n-rw-rw----+ 1 lkemp nesi99991  809 May 11 12:34 slurm-26763440.out\n-rw-rw----+ 1 lkemp nesi99991  865 May 11 12:35 slurm-26763444.out\n-rw-rw----+ 1 lkemp nesi99991  857 May 11 12:36 slurm-26763447.out\n-rw-rw----+ 1 lkemp nesi99991  829 May 11 12:36 slurm-26763448.out\n-rw-rw----+ 1 lkemp nesi99991  857 May 11 12:36 slurm-26763449.out\n-rw-rw----+ 1 lkemp nesi99991  857 May 11 12:36 slurm-26763450.out\n-rw-rw----+ 1 lkemp nesi99991  829 May 11 12:36 slurm-26763451.out\n-rw-rw----+ 1 lkemp nesi99991  829 May 11 12:36 slurm-26763452.out\n-rw-rw----+ 1 lkemp nesi99991  885 May 11 12:37 slurm-26763454.out\n-rw-rw----+ 1 lkemp nesi99991 1.8K May 11 12:34 Snakefile\n</code></pre> <p></p> <p>Let's clean this and create a dedicated folder <code>logs/slurm</code> for future log files:</p> <p>code</p> <p><pre><code># remove slurm log files\nrm *.out\n</code></pre> <pre><code># create a new folder for Slurm log files\nmkdir logs/slurm\n</code></pre></p> <p>then instruct Slurm to save its log files in it, in the profile <code>slurm/config.yaml</code> file</p> <pre><code>jobs: 20\ncluster:\n    sbatch\n        --parsable\n        --time {resources.time_min}\n        --mem {resources.mem_mb}\n        --cpus-per-task {resources.cpus}\n+       --output logs/slurm/slurm-%j-{rule}.out\n       --account nesi99991\ndefault-resources: [cpus=2, mem_mb=512, time_min=10]\ncluster-cancel: scancel\n</code></pre> <p>Note that <code>logs/slurm/slurm-%j-{rule}.out</code> contains a placeholder <code>{rule}</code>, which will be replaced by the name of the rule during the execution of the workflow.</p> <p>Finally, to improve the communication between Snakemake and Slurm, we meed an additional script translating Slurm job status for Snakemake. The <code>--cluster-status</code> option is used to tell Snakemake which script to use.</p> <p>Create an executable <code>status.py</code> file</p> <p><pre><code># create an empty file\ntouch status.py\n</code></pre> <pre><code># make it executable\nchmod +x status.py\n</code></pre></p> <ul> <li>and copy the following content in it</li> </ul> <pre><code>#!/usr/bin/env python\nimport subprocess\nimport sys\njobid = sys.argv[1]\noutput = str(subprocess.check_output(\"sacct -j %s --format State --noheader | head -1 | awk '{print $1}'\" % jobid, shell=True).strip())\nrunning_status=[\"PENDING\", \"CONFIGURING\", \"COMPLETING\", \"RUNNING\", \"SUSPENDED\"]\nif \"COMPLETED\" in output:\nprint(\"success\")\nelif any(r in output for r in running_status):\nprint(\"running\")\nelse:\nprint(\"failed\")\n</code></pre> Then modify the profile <code>slurm/config.yaml</code> file <pre><code>jobs: 20\ncluster:\n    sbatch\n        --parsable\n        --time {resources.time_min}\n        --mem {resources.mem_mb}\n        --cpus-per-task {resources.cpus}\n        --output logs/slurm/slurm-%j-{rule}.out\n        --account nesi99991\ndefault-resources: [cpus=2, mem_mb=512, time_min=10]\ncluster-cancel: scancel\n+ cluster-status: ./status.py\n</code></pre> Current slurm profile: <pre><code>jobs: 20\ncluster:\n    sbatch\n        --parsable\n        --time {resources.time_min}\n        --mem {resources.mem_mb}\n        --cpus-per-task {resources.cpus}\n        --output logs/slurm/slurm-%j-{rule}.out\n        --account nesi99991\ndefault-resources: [cpus=2, mem_mb=512, time_min=10]\ncluster-cancel: scancel\ncluster-status: ./status.py\n</code></pre> <p></p> <p>Once all of this is in place, we can:</p> <ul> <li>submit Slurm jobs with the right resources per Snakemake rule,</li> <li>cancel the workflow and Slurms jobs using CTRL-C,</li> <li>keep all slurm jobs log files in a dedicated folder,</li> <li>and make sure Snakemake reports Slurm jobs failures.</li> </ul> <p>Exercise</p> <p>Run the snakemake workflow with Slurm jobs then use <code>scancel JOBID</code> to cancel some Slurm. See how Snakemake reacts with and without the <code>status.py</code> script.</p>"},{"location":"workshop_material/04_leveling_up_your_workflow/#42-pull-out-parameters","title":"4.2 Pull out parameters","text":"Edit snakefile <pre><code># define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n\n# target OUTPUT files for the whole workflow\nrule all:\n    input:\n        \"../results/multiqc_report.html\",\n        expand([\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"], sample = SAMPLES)\n\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/{sample}_1.fastq.gz\",\n        R2 = \"../../data/{sample}_2.fastq.gz\"\n    output:\n        html = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\n        zip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\n    log:\n        \"logs/fastqc/{sample}.log\"\n    threads: 2\n    envmodules:\n        \"FastQC/0.11.9\"\n    shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} &amp;&gt; {log}\"\n\nrule multiqc:\n    input:\n        expand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\n    output:\n        \"../results/multiqc_report.html\"\n    log:\n        \"logs/multiqc/multiqc.log\"\n    envmodules:\n        \"MultiQC/1.9-gimkl-2020a-Python-3.8.2\"\n    shell:\n        \"multiqc {input} -o ../results/ &amp;&gt; {log}\"\n\nrule trim_galore:\n    input:\n        [\"../../data/{sample}_1.fastq.gz\", \"../../data/{sample}_2.fastq.gz\"]\n    output:\n        [\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"]\n+   params:\n+       \"--paired\"\n   log:\n        \"logs/trim_galore/{sample}.log\"\n    envmodules:\n        \"TrimGalore/0.6.7-gimkl-2020a-Python-3.8.2-Perl-5.30.1\"\n    threads: 2\n    resources:\n        cpus=8\n    shell:\n-       \"trim_galore {input} -o ../results/trimmed/ --paired --cores {threads} &amp;&gt; {log}\"\n+       \"trim_galore {input} -o ../results/trimmed/ {params} --cores {threads} &amp;&gt; {log}\"\n</code></pre> Current snakefile: <pre><code># define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n# target OUTPUT files for the whole workflow\nrule all:\ninput:\n\"../results/multiqc_report.html\",\nexpand([\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"], sample = SAMPLES)\n# workflow\nrule fastqc:\ninput:\nR1 = \"../../data/{sample}_1.fastq.gz\",\nR2 = \"../../data/{sample}_2.fastq.gz\"\noutput:\nhtml = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\nzip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\nlog:\n\"logs/fastqc/{sample}.log\"\nthreads: 2\nenvmodules:\n\"FastQC/0.11.9\"\nshell:\n\"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} &amp;&gt; {log}\"\nrule multiqc:\ninput:\nexpand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\noutput:\n\"../results/multiqc_report.html\"\nlog:\n\"logs/multiqc/multiqc.log\"\nenvmodules:\n\"MultiQC/1.9-gimkl-2020a-Python-3.8.2\"\nshell:\n\"multiqc {input} -o ../results/ &amp;&gt; {log}\"\nrule trim_galore:\ninput:\n[\"../../data/{sample}_1.fastq.gz\", \"../../data/{sample}_2.fastq.gz\"]\noutput:\n[\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"]\nparams:\n\"--paired\"\nlog:\n\"logs/trim_galore/{sample}.log\"\nenvmodules:\n\"TrimGalore/0.6.7-gimkl-2020a-Python-3.8.2-Perl-5.30.1\"\nthreads: 2\nresources:\ncpus=8\nshell:\n\"trim_galore {input} -o ../results/trimmed/ {params} --cores {threads} &amp;&gt; {log}\"\n</code></pre> <p>Run a dryrun to check it works</p> <p><pre><code># remove output of last run\nrm -r ../results/*\n</code></pre> <pre><code># run dryrun again\nsnakemake --dryrun --profile slurm --use-envmodules\n</code></pre></p>"},{"location":"workshop_material/04_leveling_up_your_workflow/#43-pull-out-user-configurable-options","title":"4.3 Pull out user configurable options","text":"<p>We can separate the user configurable options away from the workflow. This supports reproducibility by minimising the chance the user makes changes to the core workflow.</p> <p>Create a configuration file in a new directory <code>config/</code></p> <p>File structure:</p> <pre><code>demo_workflow/\n      |_______results/\n      |_______workflow/\n      |          |_______logs/\n      |          |_______slurm/\n      |          |_______Snakefile\n      |          |_______status.py\n      |_______config\n                 |_______config.yaml\n</code></pre> <p>code</p> <p><pre><code># create config directory\nmkdir ../config\n</code></pre> <pre><code># create configuration file\ntouch ../config/config.yaml\n</code></pre></p> <p>Now we need to pull out the parameters the user would likely need to configure. Let's give the user the option to pass any parameters they like to fastqc. In our <code>../config/config.yaml</code> file, add the configuration options and add a couple flags to be passed to fastqc and multiqc:</p> <pre><code># set software parameters for...\nPARAMS:\n# ... fastqc\nFASTQC: \"--kmers 5\"\n# ... multiqc\nMULTIQC: \"--flat\"\n</code></pre> Edit snakefile : In the Snakefile, tell Snakemake to grab the variables <code>PARAMS</code> from <code>../config/config.yaml</code> <pre><code># define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n\n# target OUTPUT files for the whole workflow\nrule all:\n    input:\n        \"../results/multiqc_report.html\",\n        expand([\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"], sample = SAMPLES)\n\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/{sample}_1.fastq.gz\",\n        R2 = \"../../data/{sample}_2.fastq.gz\"\n    output:\n        html = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\n        zip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\n+   params:\n+       fastqc_params = config['PARAMS']['FASTQC']\n   log:\n        \"logs/fastqc/{sample}.log\"\n    threads: 2\n    envmodules:\n        \"FastQC/0.11.9\"\n    shell:\n-       \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} &amp;&gt; {log}\"\n+       \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} {params.fastqc_params} &amp;&gt; {log}\"\nrule multiqc:\n    input:\n        expand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\n    output:\n        \"../results/multiqc_report.html\"\n+   params:\n+       multiqc_params = config['PARAMS']['MULTIQC']\n   log:\n        \"logs/multiqc/multiqc.log\"\n    envmodules:\n        \"MultiQC/1.9-gimkl-2020a-Python-3.8.2\"\n    shell:\n-       \"multiqc {input} -o ../results/ &amp;&gt; {log}\"\n+       \"multiqc {input} -o ../results/ {params.multiqc_params} &amp;&gt; {log}\"\nrule trim_galore:\n    input:\n        [\"../../data/{sample}_1.fastq.gz\", \"../../data/{sample}_2.fastq.gz\"]\n    output:\n        [\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"]\n    params:\n        \"--paired\"\n    log:\n        \"logs/trim_galore/{sample}.log\"\n    envmodules:\n        \"TrimGalore/0.6.7-gimkl-2020a-Python-3.8.2-Perl-5.30.1\"\n    threads: 2\n    resources:\n        cpus=8\n    shell:\n        \"trim_galore {input} -o ../results/trimmed/ {params} --cores {threads} &amp;&gt; {log}\"\n</code></pre> Current snakefile <pre><code># define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n# target OUTPUT files for the whole workflow\nrule all:\ninput:\n\"../results/multiqc_report.html\",\nexpand([\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"], sample = SAMPLES)\n# workflow\nrule fastqc:\ninput:\nR1 = \"../../data/{sample}_1.fastq.gz\",\nR2 = \"../../data/{sample}_2.fastq.gz\"\noutput:\nhtml = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\nzip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\nparams:\nfastqc_params = config['PARAMS']['FASTQC']\nlog:\n\"logs/fastqc/{sample}.log\"\nthreads: 2\nenvmodules:\n\"FastQC/0.11.9\"\nshell:\n\"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} {params.fastqc_params} &amp;&gt; {log}\"\nrule multiqc:\ninput:\nexpand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\noutput:\n\"../results/multiqc_report.html\"\nparams:\nmultiqc_params = config['PARAMS']['MULTIQC']\nlog:\n\"logs/multiqc/multiqc.log\"\nenvmodules:\n\"MultiQC/1.9-gimkl-2020a-Python-3.8.2\"\nshell:\n\"multiqc {input} -o ../results/ {params.multiqc_params} &amp;&gt; {log}\"\nrule trim_galore:\ninput:\n[\"../../data/{sample}_1.fastq.gz\", \"../../data/{sample}_2.fastq.gz\"]\noutput:\n[\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"]\nparams:\n\"--paired\"\nlog:\n\"logs/trim_galore/{sample}.log\"\nenvmodules:\n\"TrimGalore/0.6.7-gimkl-2020a-Python-3.8.2-Perl-5.30.1\"\nthreads: 2\nresources:\ncpus=8\nshell:\n\"trim_galore {input} -o ../results/trimmed/ {params} --cores {threads} &amp;&gt; {log}\"\n</code></pre> <p></p> <p>Let's use our configuration file! Run workflow again:</p> <p><pre><code># remove output of last run\nrm -r ../results/*\n</code></pre> <pre><code># run dryrun/run again\nsnakemake --dryrun --profile slurm --use-envmodules\n</code></pre> <pre><code>snakemake --profile slurm --use-envmodules\n</code></pre></p> Didn't work? My error: <pre><code>KeyError in line 19 of /scale_wlg_persistent/filesets/project/nesi99991/snakemake20220512/lkemp/snakemake_workshop/demo_workflow/workflow/Snakefile:\n'PARAMS'\nFile \"/scale_wlg_persistent/filesets/project/nesi99991/snakemake20220512/lkemp/snakemake_workshop/demo_workflow/workflow/Snakefile\", line 19, in &lt;module&gt;\n</code></pre> <p></p> <p>Snakemake can't find our 'Key' - we haven't told Snakemake where our config file is so it can't find our config variables. We can do this by passing the location of our config file to the <code>--configfile</code> flag</p> <p>code</p> <p><pre><code># remove output of last run\nrm -r ../results/\n</code></pre> <pre><code># run dryrun/run again\n- snakemake --dryrun --profile slurm --use-envmodules\n- snakemake --profile slurm --use-envmodules\n+ snakemake --dryrun --profile slurm --use-envmodules --configfile ../config/config.yaml\n+ snakemake --profile slurm --use-envmodules --configfile ../config/config.yaml\n</code></pre></p> <p>Alternatively, we can define our config file in our Snakefile in a situation where the configuration file is likely to always be named the same and be in the exact same location <code>../config/config.yaml</code> and you don't need the flexibility for the user to specify their own configuration files:</p> Edit snakefile <pre><code># define our configuration file\n+ configfile: \"../config/config.yaml\"\n# define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n\n# target OUTPUT files for the whole workflow\nrule all:\n    input:\n        \"../results/multiqc_report.html\",\n        expand([\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"], sample = SAMPLES)\n\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/{sample}_1.fastq.gz\",\n        R2 = \"../../data/{sample}_2.fastq.gz\"\n    output:\n        html = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\n        zip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\n    params:\n        fastqc_params = config['PARAMS']['FASTQC']\n    log:\n        \"logs/fastqc/{sample}.log\"\n    threads: 2\n    envmodules:\n        \"FastQC/0.11.9\"\n    shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} {params.fastqc_params} &amp;&gt; {log}\"\n\nrule multiqc:\n    input:\n        expand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\n    output:\n        \"../results/multiqc_report.html\"\n    params:\n        multiqc_params = config['PARAMS']['MULTIQC']\n    log:\n        \"logs/multiqc/multiqc.log\"\n    envmodules:\n        \"MultiQC/1.9-gimkl-2020a-Python-3.8.2\"\n    shell:\n        \"multiqc {input} -o ../results/ {params.multiqc_params} &amp;&gt; {log}\"\n\nrule trim_galore:\n    input:\n        [\"../../data/{sample}_1.fastq.gz\", \"../../data/{sample}_2.fastq.gz\"]\n    output:\n        [\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"]\n    params:\n        \"--paired\"\n    log:\n        \"logs/trim_galore/{sample}.log\"\n    envmodules:\n        \"TrimGalore/0.6.7-gimkl-2020a-Python-3.8.2-Perl-5.30.1\"\n    threads: 2\n    resources:\n        cpus=8\n    shell:\n        \"trim_galore {input} -o ../results/trimmed/ {params} --cores {threads} &amp;&gt; {log}\"\n</code></pre> Current snakefile: <pre><code># define our configuration file\nconfigfile: \"../config/config.yaml\"\n# define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n# target OUTPUT files for the whole workflow\nrule all:\ninput:\n\"../results/multiqc_report.html\",\nexpand([\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"], sample = SAMPLES)\n# workflow\nrule fastqc:\ninput:\nR1 = \"../../data/{sample}_1.fastq.gz\",\nR2 = \"../../data/{sample}_2.fastq.gz\"\noutput:\nhtml = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\nzip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\nparams:\nfastqc_params = config['PARAMS']['FASTQC']\nlog:\n\"logs/fastqc/{sample}.log\"\nthreads: 2\nenvmodules:\n\"FastQC/0.11.9\"\nshell:\n\"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} {params.fastqc_params} &amp;&gt; {log}\"\nrule multiqc:\ninput:\nexpand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\noutput:\n\"../results/multiqc_report.html\"\nparams:\nmultiqc_params = config['PARAMS']['MULTIQC']\nlog:\n\"logs/multiqc/multiqc.log\"\nenvmodules:\n\"MultiQC/1.9-gimkl-2020a-Python-3.8.2\"\nshell:\n\"multiqc {input} -o ../results/ {params.multiqc_params} &amp;&gt; {log}\"\nrule trim_galore:\ninput:\n[\"../../data/{sample}_1.fastq.gz\", \"../../data/{sample}_2.fastq.gz\"]\noutput:\n[\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"]\nparams:\n\"--paired\"\nlog:\n\"logs/trim_galore/{sample}.log\"\nenvmodules:\n\"TrimGalore/0.6.7-gimkl-2020a-Python-3.8.2-Perl-5.30.1\"\nthreads: 2\nresources:\ncpus=8\nshell:\n\"trim_galore {input} -o ../results/trimmed/ {params} --cores {threads} &amp;&gt; {log}\"\n</code></pre> <p></p> <p>Then we don't need to specify where the configuration file is on the command line</p> <p>code</p> <p><pre><code># remove output of last run\nrm -r ../results/*\n</code></pre> <pre><code># run dryrun/run again\n- snakemake --dryrun --profile slurm --use-envmodules --configfile ../config/config.yaml\n- snakemake --profile slurm --use-envmodules --configfile ../config/config.yaml\n+ snakemake --dryrun --profile slurm --use-envmodules\n+ snakemake --profile slurm --use-envmodules\n</code></pre></p>"},{"location":"workshop_material/04_leveling_up_your_workflow/#44-leave-messages-for-the-user","title":"4.4 Leave messages for the user","text":"<p>We can provide the user of our workflow more information on what is happening at each stage/rule of our workflow via the <code>message:</code> directive. We are able to call many variables such as:</p> <ul> <li>Input and output files <code>{input}</code> and <code>{output}</code></li> <li>Specific input and output files such as <code>{input.R1}</code></li> <li>Our <code>{params}</code>, <code>{log}</code> and <code>{threads}</code> directives</li> </ul> Edit snakefile <pre><code># define our configuration file\nconfigfile: \"../config/config.yaml\"\n\n# define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n\n# target OUTPUT files for the whole workflow\nrule all:\n    input:\n        \"../results/multiqc_report.html\",\n        expand([\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"], sample = SAMPLES)\n\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/{sample}_1.fastq.gz\",\n        R2 = \"../../data/{sample}_2.fastq.gz\"\n    output:\n        html = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\n        zip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\n    params:\n        fastqc_params = config['PARAMS']['FASTQC']\n    log:\n        \"logs/fastqc/{sample}.log\"\n    threads: 2\n    envmodules:\n        \"FastQC/0.11.9\"\n+   message:\n+       \"Undertaking quality control checks {input}\"\n   shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} {params.fastqc_params} &amp;&gt; {log}\"\n\nrule multiqc:\n    input:\n        expand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\n    output:\n        \"../results/multiqc_report.html\"\n    params:\n        multiqc_params = config['PARAMS']['MULTIQC']\n    log:\n        \"logs/multiqc/multiqc.log\"\n    envmodules:\n        \"MultiQC/1.9-gimkl-2020a-Python-3.8.2\"\n+   message:\n+       \"Compiling a HTML report for quality control checks. Writing to {output}.\"\n   shell:\n        \"multiqc {input} -o ../results/ {params.multiqc_params} &amp;&gt; {log}\"\n\nrule trim_galore:\n    input:\n        [\"../../data/{sample}_1.fastq.gz\", \"../../data/{sample}_2.fastq.gz\"]\n    output:\n        [\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"]\n    params:\n        \"--paired\"\n    log:\n        \"logs/trim_galore/{sample}.log\"\n    envmodules:\n        \"TrimGalore/0.6.7-gimkl-2020a-Python-3.8.2-Perl-5.30.1\"\n    threads: 2\n    resources:\n        cpus=8\n+   message:\n+       \"Trimming using these parameter: {params}. Writing logs to {log}. Using {threads} threads.\"\n   shell:\n        \"trim_galore {input} -o ../results/trimmed/ {params} --cores {threads} &amp;&gt; {log}\"\n</code></pre> Current snakefile: <pre><code># define our configuration file\nconfigfile: \"../config/config.yaml\"\n# define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n# target OUTPUT files for the whole workflow\nrule all:\ninput:\n\"../results/multiqc_report.html\",\nexpand([\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"], sample = SAMPLES)\n# workflow\nrule fastqc:\ninput:\nR1 = \"../../data/{sample}_1.fastq.gz\",\nR2 = \"../../data/{sample}_2.fastq.gz\"\noutput:\nhtml = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\nzip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\nparams:\nfastqc_params = config['PARAMS']['FASTQC']\nlog:\n\"logs/fastqc/{sample}.log\"\nthreads: 2\nenvmodules:\n\"FastQC/0.11.9\"\nmessage:\n\"Undertaking quality control checks {input}\"\nshell:\n\"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} {params.fastqc_params} &amp;&gt; {log}\"\nrule multiqc:\ninput:\nexpand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\noutput:\n\"../results/multiqc_report.html\"\nparams:\nmultiqc_params = config['PARAMS']['MULTIQC']\nlog:\n\"logs/multiqc/multiqc.log\"\nenvmodules:\n\"MultiQC/1.9-gimkl-2020a-Python-3.8.2\"\nmessage:\n\"Compiling a HTML report for quality control checks. Writing to {output}.\"\nshell:\n\"multiqc {input} -o ../results/ {params.multiqc_params} &amp;&gt; {log}\"\nrule trim_galore:\ninput:\n[\"../../data/{sample}_1.fastq.gz\", \"../../data/{sample}_2.fastq.gz\"]\noutput:\n[\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"]\nparams:\n\"--paired\"\nlog:\n\"logs/trim_galore/{sample}.log\"\nenvmodules:\n\"TrimGalore/0.6.7-gimkl-2020a-Python-3.8.2-Perl-5.30.1\"\nthreads: 2\nresources:\ncpus=8\nmessage:\n\"Trimming using these parameter: {params}. Writing logs to {log}. Using {threads} threads.\"\nshell:\n\"trim_galore {input} -o ../results/trimmed/ {params} --cores {threads} &amp;&gt; {log}\"\n</code></pre> <p></p> <p>code</p> <pre><code># remove output of last run\nrm -r ../results/*\n</code></pre> <pre><code># run dryrun/run again\nsnakemake --dryrun --profile slurm --use-envmodules\nsnakemake --profile slurm --use-envmodules\n</code></pre> <ul> <li>Now our messages are printed to the screen as our workflow runs</li> </ul> output <pre><code>Building DAG of jobs...\nUsing shell: /usr/bin/bash\nProvided cluster nodes: 20\nJob stats:\njob            count    min threads    max threads\n-----------  -------  -------------  -------------\nall                1              1              1\nfastqc             3              2              2\nmultiqc            1              1              1\ntrim_galore        3              2              2\ntotal              8              1              2\nSelect jobs to execute...\n\n[Wed May 11 13:20:52 2022]\nJob 4: Undertaking quality control checks ../../data/NA24694_1.fastq.gz ../../data/NA24694_2.fastq.gz\n\nSubmitted job 4 with external jobid '26763840'.\n\n[Wed May 11 13:20:52 2022]\nJob 6: Trimming using these parameter: --paired. Writing logs to logs/trim_galore/NA24695.log. Using 2 threads.\n\nSubmitted job 6 with external jobid '26763841'.\n\n[Wed May 11 13:20:52 2022]\nJob 2: Undertaking quality control checks ../../data/NA24631_1.fastq.gz ../../data/NA24631_2.fastq.gz\n\nSubmitted job 2 with external jobid '26763842'.\n\n[Wed May 11 13:20:52 2022]\nJob 3: Undertaking quality control checks ../../data/NA24695_1.fastq.gz ../../data/NA24695_2.fastq.gz\n\nSubmitted job 3 with external jobid '26763843'.\n\n[Wed May 11 13:20:52 2022]\nJob 5: Trimming using these parameter: --paired. Writing logs to logs/trim_galore/NA24631.log. Using 2 threads.\n\nSubmitted job 5 with external jobid '26763844'.\n\n[Wed May 11 13:20:52 2022]\nJob 7: Trimming using these parameter: --paired. Writing logs to logs/trim_galore/NA24694.log. Using 2 threads.\n\nSubmitted job 7 with external jobid '26763845'.\n[Wed May 11 13:22:08 2022]\nFinished job 4.\n1 of 8 steps (12%) done\n[Wed May 11 13:22:08 2022]\nFinished job 6.\n2 of 8 steps (25%) done\n[Wed May 11 13:22:08 2022]\nFinished job 2.\n3 of 8 steps (38%) done\n[Wed May 11 13:22:08 2022]\nFinished job 3.\n4 of 8 steps (50%) done\nSelect jobs to execute...\n\n[Wed May 11 13:22:09 2022]\nJob 1: Compiling a HTML report for quality control checks. Writing to ../results/multiqc_report.html.\n\nSubmitted job 1 with external jobid '26763848'.\n[Wed May 11 13:22:09 2022]\nFinished job 5.\n5 of 8 steps (62%) done\n[Wed May 11 13:22:09 2022]\nFinished job 7.\n6 of 8 steps (75%) done\n[Wed May 11 13:24:21 2022]\nFinished job 1.\n7 of 8 steps (88%) done\nSelect jobs to execute...\n\n[Wed May 11 13:24:21 2022]\nlocalrule all:\n    input: ../results/multiqc_report.html, ../results/trimmed/NA24631_1_val_1.fq.gz, ../results/trimmed/NA24695_1_val_1.fq.gz, ../results/trimmed/NA24694_1_val_1.fq.gz, ../results/trimmed/NA24631_2_val_2.fq.gz, ../results/trimmed/NA24695_2_val_2.fq.gz, ../results/trimmed/NA24694_2_val_2.fq.gz\n    jobid: 0\nresources: mem_mb=512, disk_mb=1000, tmpdir=/dev/shm/jobs/26763281, cpus=2, time_min=10\n[Wed May 11 13:24:21 2022]\nFinished job 0.\n8 of 8 steps (100%) done\nComplete log: .snakemake/log/2022-05-11T132052.454902.snakemake.log\n</code></pre> <p></p>"},{"location":"workshop_material/04_leveling_up_your_workflow/#45-create-temporary-files","title":"4.5 Create temporary files","text":"<p>In our workflow, we are likely to be creating files that we don't want, but are used or produced by our workflow (intermediate files). We can mark such files as temporary so Snakemake will remove the file once it doesn't need to use it anymore.</p> <p>For example, we might not want to keep our fastqc output files since our multiqc report merges all of our fastqc reports for each sample into one report. Let's have a look at the files currently produced by our workflow with:</p> <p>code</p> <pre><code>ls -lh ../results/fastqc/\n</code></pre> output <pre><code>total 4.5M\n-rw-rw----+ 1 lkemp nesi99991 250K May 11 13:22 NA24631_1_fastqc.html\n-rw-rw----+ 1 lkemp nesi99991 327K May 11 13:22 NA24631_1_fastqc.zip\n-rw-rw----+ 1 lkemp nesi99991 249K May 11 13:22 NA24631_2_fastqc.html\n-rw-rw----+ 1 lkemp nesi99991 327K May 11 13:22 NA24631_2_fastqc.zip\n-rw-rw----+ 1 lkemp nesi99991 254K May 11 13:22 NA24694_1_fastqc.html\n-rw-rw----+ 1 lkemp nesi99991 334K May 11 13:22 NA24694_1_fastqc.zip\n-rw-rw----+ 1 lkemp nesi99991 250K May 11 13:22 NA24694_2_fastqc.html\n-rw-rw----+ 1 lkemp nesi99991 328K May 11 13:22 NA24694_2_fastqc.zip\n-rw-rw----+ 1 lkemp nesi99991 252K May 11 13:22 NA24695_1_fastqc.html\n-rw-rw----+ 1 lkemp nesi99991 328K May 11 13:22 NA24695_1_fastqc.zip\n-rw-rw----+ 1 lkemp nesi99991 253K May 11 13:22 NA24695_2_fastqc.html\n-rw-rw----+ 1 lkemp nesi99991 330K May 11 13:22 NA24695_2_fastqc.zip\n</code></pre> <p></p> <p>Let's mark all the trimmed fastq files as temporary in our Snakefile by wrapping it up in the <code>temp()</code> function</p> Edit snakefile <pre><code># define our configuration file\nconfigfile: \"../config/config.yaml\"\n\n# define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n\n# target OUTPUT files for the whole workflow\nrule all:\n    input:\n        \"../results/multiqc_report.html\",\n        expand([\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"], sample = SAMPLES)\n\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/{sample}_1.fastq.gz\",\n        R2 = \"../../data/{sample}_2.fastq.gz\"\n    output:\n-       html = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\n+       html = temp([\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"]),\n       zip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\n    params:\n        fastqc_params = config['PARAMS']['FASTQC']\n    log:\n        \"logs/fastqc/{sample}.log\"\n    threads: 2\n    envmodules:\n        \"FastQC/0.11.9\"\n    message:\n        \"Undertaking quality control checks {input}\"\n    shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} {params.fastqc_params} &amp;&gt; {log}\"\n\nrule multiqc:\n    input:\n        expand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\n    output:\n        \"../results/multiqc_report.html\"\n    params:\n        multiqc_params = config['PARAMS']['MULTIQC']\n    log:\n        \"logs/multiqc/multiqc.log\"\n    envmodules:\n        \"MultiQC/1.9-gimkl-2020a-Python-3.8.2\"\n    message:\n        \"Compiling a HTML report for quality control checks. Writing to {output}.\"\n    shell:\n        \"multiqc {input} -o ../results/ {params.multiqc_params} &amp;&gt; {log}\"\n\nrule trim_galore:\n    input:\n        [\"../../data/{sample}_1.fastq.gz\", \"../../data/{sample}_2.fastq.gz\"]\n    output:\n        [\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"]\n    params:\n        \"--paired\"\n    log:\n        \"logs/trim_galore/{sample}.log\"\n    envmodules:\n        \"TrimGalore/0.6.7-gimkl-2020a-Python-3.8.2-Perl-5.30.1\"\n    threads: 2\n    resources:\n        cpus=8\n    message:\n        \"Trimming using these parameter: {params}. Writing logs to {log}. Using {threads} threads.\"\n    shell:\n        \"trim_galore {input} -o ../results/trimmed/ {params} --cores {threads} &amp;&gt; {log}\"\n</code></pre> Current snakefile: <pre><code># define our configuration file\nconfigfile: \"../config/config.yaml\"\n# define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n# target OUTPUT files for the whole workflow\nrule all:\ninput:\n\"../results/multiqc_report.html\",\nexpand([\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"], sample = SAMPLES)\n# workflow\nrule fastqc:\ninput:\nR1 = \"../../data/{sample}_1.fastq.gz\",\nR2 = \"../../data/{sample}_2.fastq.gz\"\noutput:\nhtml = temp([\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"]),\nzip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\nparams:\nfastqc_params = config['PARAMS']['FASTQC']\nlog:\n\"logs/fastqc/{sample}.log\"\nthreads: 2\nenvmodules:\n\"FastQC/0.11.9\"\nmessage:\n\"Undertaking quality control checks {input}\"\nshell:\n\"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} {params.fastqc_params} &amp;&gt; {log}\"\nrule multiqc:\ninput:\nexpand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\noutput:\n\"../results/multiqc_report.html\"\nparams:\nmultiqc_params = config['PARAMS']['MULTIQC']\nlog:\n\"logs/multiqc/multiqc.log\"\nenvmodules:\n\"MultiQC/1.9-gimkl-2020a-Python-3.8.2\"\nmessage:\n\"Compiling a HTML report for quality control checks. Writing to {output}.\"\nshell:\n\"multiqc {input} -o ../results/ {params.multiqc_params} &amp;&gt; {log}\"\nrule trim_galore:\ninput:\n[\"../../data/{sample}_1.fastq.gz\", \"../../data/{sample}_2.fastq.gz\"]\noutput:\n[\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"]\nparams:\n\"--paired\"\nlog:\n\"logs/trim_galore/{sample}.log\"\nenvmodules:\n\"TrimGalore/0.6.7-gimkl-2020a-Python-3.8.2-Perl-5.30.1\"\nthreads: 2\nresources:\ncpus=8\nmessage:\n\"Trimming using these parameter: {params}. Writing logs to {log}. Using {threads} threads.\"\nshell:\n\"trim_galore {input} -o ../results/trimmed/ {params} --cores {threads} &amp;&gt; {log}\"\n</code></pre> <p></p> <p>code</p> <p><pre><code># remove output of last run\nrm -r ../results/*\n</code></pre> <pre><code># run dryrun/run again\nsnakemake --dryrun --profile slurm --use-envmodules\nsnakemake --profile slurm --use-envmodules\n</code></pre></p> <p>Now when we have a look at the <code>../results/fastqc/</code> directory with:</p> <pre><code>ls -lh ../results/fastqc/\n</code></pre> <ul> <li>These html files have been removed once Snakemake no longer needs the files for another rule/operation, and we've saved some space on our computer (from 4.5 megabytes to 3 megabytes in this directory).</li> </ul> output <pre><code>total 3.0M\n-rw-rw----+ 1 lkemp nesi99991 327K May 11 13:26 NA24631_1_fastqc.zip\n-rw-rw----+ 1 lkemp nesi99991 327K May 11 13:26 NA24631_2_fastqc.zip\n-rw-rw----+ 1 lkemp nesi99991 334K May 11 13:26 NA24694_1_fastqc.zip\n-rw-rw----+ 1 lkemp nesi99991 328K May 11 13:26 NA24694_2_fastqc.zip\n-rw-rw----+ 1 lkemp nesi99991 328K May 11 13:26 NA24695_1_fastqc.zip\n-rw-rw----+ 1 lkemp nesi99991 330K May 11 13:26 NA24695_2_fastqc.zip\n</code></pre> <p></p> <p>This becomes particularly important when our data become big data, since we don't want to keep any massive intermediate output files that we don't need. Otherwise this can start to clog up the memory on our computer. It ensures our workflow is scalable when our data becomes big data.</p>"},{"location":"workshop_material/04_leveling_up_your_workflow/#46-generating-a-snakemake-report","title":"4.6 Generating a snakemake report","text":"<p>With Snakemake, we can automatically generate detailed self-contained HTML reports after we run our workflow with the following command:</p> <p>code</p> <pre><code>snakemake --report ../results/snakemake_report.html\n</code></pre> <p>Note</p> <p>you won't be able to view a rendered version of this html while it is on the remote server, however after you transfer it to your local computer you should be able to view it in your web browser*</p> <p>In our report:</p> <ul> <li>We get an interactive version of our directed acyclic graph (DAG).</li> <li>When you click on a node in the DAG, the input and output files are fully outlined, the exact software used and the exact shell command that was run.</li> <li>You are also provided with runtime information under the <code>Statistics</code> tab outlining how long each rule/sample ran for, and the date/time each file was created.</li> </ul> <p>My report</p> <p></p> <p></p> <p>These reports are highly configurable, have a look at an example of what can be done with a report here</p> <p>See more information on creating Snakemake reports in the Snakemake documentation</p>"},{"location":"workshop_material/04_leveling_up_your_workflow/#47-linting-your-workflow","title":"4.7 Linting your workflow","text":"<p>Snakemake has a built in linter to support you building best practice workflows, let's try it out:</p> <p>code</p> <pre><code>snakemake --lint\n</code></pre> output <pre><code>Lints for rule fastqc (line 21, /scale_wlg_persistent/filesets/project/nesi99991/snakemake20220512/lkemp/snakemake_workshop/demo_workflow/workflow/Snakefile):\n    * Additionally specify a conda environment or container for each rule, environment modules are not enough:\n      While environment modules allow to document and deploy the required software on a certain platform, they lock your workflow in there, disabling easy reproducibility on\n      other machines that don't have exactly the same environment modules. Hence env modules (which might be beneficial in certain cluster environments), should allways be\n      complemented with equivalent conda environments.\n      Also see:\n      https://snakemake.readthedocs.io/en/latest/snakefiles/deployment.html#integrated-package-management\n      https://snakemake.readthedocs.io/en/latest/snakefiles/deployment.html#running-jobs-in-containers\nLints for rule multiqc (line 61, /scale_wlg_persistent/filesets/project/nesi99991/snakemake20220512/lkemp/snakemake_workshop/demo_workflow/workflow/Snakefile):\n    * Additionally specify a conda environment or container for each rule, environment modules are not enough:\n      While environment modules allow to document and deploy the required software on a certain platform, they lock your workflow in there, disabling easy reproducibility on\n      other machines that don't have exactly the same environment modules. Hence env modules (which might be beneficial in certain cluster environments), should allways be\n      complemented with equivalent conda environments.\n      Also see:\n      https://snakemake.readthedocs.io/en/latest/snakefiles/deployment.html#integrated-package-management\n      https://snakemake.readthedocs.io/en/latest/snakefiles/deployment.html#running-jobs-in-containers\n\nLints for rule trim_galore (line 96, /scale_wlg_persistent/filesets/project/nesi99991/snakemake20220512/lkemp/snakemake_workshop/demo_workflow/workflow/Snakefile):\n    * Additionally specify a conda environment or container for each rule, environment modules are not enough:\n      While environment modules allow to document and deploy the required software on a certain platform, they lock your workflow in there, disabling easy reproducibility on\n      other machines that don't have exactly the same environment modules. Hence env modules (which might be beneficial in certain cluster environments), should allways be\n      complemented with equivalent conda environments.\n      Also see:\n      https://snakemake.readthedocs.io/en/latest/snakefiles/deployment.html#integrated-package-management\n      https://snakemake.readthedocs.io/en/latest/snakefiles/deployment.html#running-jobs-in-containers\n</code></pre> <p></p> <p>We have a few things we could improve in our workflow!</p> <p>Writing a best practice workflow is more important than having Marie Kondo level tidiness, it increases the chance your workflow will continue to be used and maintained by others (and ourselves), making the code we write useful (it's exciting seeing someone else using your code!). If your workflow was used in scientific research, it makes your workflow accessible for people to reproduce your research findings; it isn't going to be a nightmare for them to run and they are more likely to try and have success doing so.</p> <p>Read more about the best practices for Snakemake</p>"},{"location":"workshop_material/04_leveling_up_your_workflow/#takeaways","title":"Takeaways","text":"<ul> <li>Pull out your parameters and put them in <code>params:</code> directive</li> <li>Pulling the user configurable options away from the core workflow will support reproducibility by reducing the chance of changes to the core workflow</li> <li>Leaving messages for the user of your workflow will help them understand what is happening at each stage and follow the workflows progress</li> <li>Mark files you won't need once the workflow completes to reduce the memory usage - particularly when dealing with big data</li> <li>Generate a snakemake report to get a summary of the workflow run - these are highly configurable</li> <li>Lint your workflow and check it complies with best practices - this supports reproducibility and portability</li> <li>There is so much more to explore, such as creating modular workflows, automatically grabbing remote files from places like Google Cloud Storage and Dropbox, run various types of scripts such as python scripts, R and RMarkdown scripts and Jupyter Notebooks</li> </ul>"},{"location":"workshop_material/04_leveling_up_your_workflow/#summary-commands","title":"Summary commands","text":"<ul> <li>Use the parameter directive (<code>params</code>) to keep the parameters and flags of your programs separate from your shell command, for example:</li> </ul> <pre><code>params:\n    \"--paired\"\n</code></pre> <ul> <li>Run your snakemake workflow (using environment modules to load your software AND with a configuration file) with:</li> </ul> <pre><code>snakemake --cores 2 --use-envmodules --configfile ../config/config.yaml\n</code></pre> <ul> <li>Alternatively, define your config file in the Snakefile:</li> </ul> <pre><code>configfile: \"../config/config.yaml\"\n</code></pre> <ul> <li>Use the <code>message</code> directive to provide information to the user on what is happening real time, for example:</li> </ul> <pre><code>message:\n    \"Undertaking quality control checks {input}\"\n</code></pre> <ul> <li>Mark temporary files to remove (once they are no longer needed by the workflow) with <code>temp()</code>, for example:</li> </ul> <pre><code>temp([\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"])\n</code></pre> <ul> <li>Create a basic interactive Snakemake report after running your workflow with:</li> </ul> <pre><code>snakemake --report ../results/snakemake_report.html\n</code></pre>"},{"location":"workshop_material/04_leveling_up_your_workflow/#our-final-snakemake-workflow","title":"Our final snakemake workflow!","text":"<p>See leveled_up_demo_workflow for the final Snakemake workflow we've created up to this point</p> <p>Back to homepage</p>"},{"location":"workshop_material/05_we_want_more/","title":"05 - We want more!","text":"<ul> <li> <p>Before going full ham creating your own Snakemake workflow, it's a good idea to see if someone has already made what you were thinking of creating! Have a look at the workflows available in the Snakemake workflow catalog. Also see all the results that were returned by searching \"rna pipeline snakemake\" in github. If it doesn't do exactly what you want, you can fork it and adapt the workflow to your use</p> </li> <li> <p>Snakemake provides incredible documentation and there is an insane amount more you can do with Snakemake beyond what was covered here</p> </li> <li> <p>Snakemake also provides a great tutorial (and a short tutorial for those pressed for time)</p> </li> <li> <p>Here is a fully established pipeline being used in production use that extends on the workflow we created in this workshop: https://github.com/ESR-NZ/human_genomics_pipeline</p> </li> </ul> <p>Back to homepage</p>"},{"location":"workshop_material/supplementary/99_appendix_setup_on_your_machine/","title":"Supp. 1 - Setup on your machine using conda","text":""},{"location":"workshop_material/supplementary/99_appendix_setup_on_your_machine/#install-miniconda","title":"Install Miniconda","text":"<p>For this workshop, we will analyse our data using various software. However, the only software we will need to manually install is Miniconda.</p>"},{"location":"workshop_material/supplementary/99_appendix_setup_on_your_machine/#check-your-os","title":"Check your OS","text":"<p>If you already use Linux or MacOS X, great! Ignore this paragraph!. If you use Windows, setup a Linux virtual machine (VM) with Vagrant (see instructions on how to do this here).</p>"},{"location":"workshop_material/supplementary/99_appendix_setup_on_your_machine/#installing-miniconda","title":"Installing miniconda","text":"<p>Information on how to install Miniconda can be found on their website. Snakemake also provides information on installing Miniconda in their documentation</p> <p>Once miniconda is installed, set up your channels (channels are locations where packages/software are can be installed from)</p> <pre><code>conda config --add channels defaults\nconda config --add channels bioconda\nconda config --add channels conda-forge\n</code></pre>"},{"location":"workshop_material/supplementary/99_appendix_setup_on_your_machine/#create-a-conda-environment","title":"Create a conda environment","text":"<p>With Miniconda, we can create a conda environment which acts as a space contained from the rest of the machine in which our workflow will automatically install all the necessary software it uses, supporting the portability and reproducibility of your workflow.</p> <p>Create a conda environment (called <code>snakemake_env</code>) that has Snakemake installed (and all it's dependant software) and git (which will be used to clone this repository later)</p> <pre><code>conda create -n snakemake_env snakemake mamba git\n</code></pre> <p>Respond yes to the following prompt to install the necessary software in the new conda environment:</p> <pre><code>Proceed ([y]/n)?\n</code></pre> <p>Note. this installed Snakemake version 6.7.0 for me, you can use the same version this workshop was created with <code>conda create -n snakemake_env snakemake=6.7.0</code></p> <p>Activate the conda environment we just created</p> <pre><code>conda activate snakemake_env\n</code></pre> <p>Now we can see which conda environment we are in on the command line, <code>(base)</code> has been replaced with <code>(snakemake_env)</code></p> <pre><code>(snakemake_env) orac$ </code></pre> <p>Snakemake has been installed within your <code>snakemake_env</code> environment, so you won't be able to see or use your Snakemake install unless you are within this environment</p>"},{"location":"workshop_material/supplementary/99_appendix_setup_on_your_machine/#clone-this-repo","title":"Clone this repo","text":"<p>Clone this repo with the following:</p> <pre><code>git clone https://github.com/nesi/snakemake_workshop.git\ncd snakemake_workshop\n</code></pre> <p>See the Git Guides for information on cloning a repo</p>"},{"location":"workshop_material/supplementary/99_appendix_use_conda_environments/","title":"Supp. 2 - Use conda environments","text":""},{"location":"workshop_material/supplementary/99_appendix_use_conda_environments/#introduction","title":"Introduction","text":"<p>Conda environments can be used as a substitute to environment modules to run a rule in a self-contained and reproducible environment. This page presents an alternative version of Section 3.09: Run using environment modules.</p>"},{"location":"workshop_material/supplementary/99_appendix_use_conda_environments/#activate-conda-on-nesi","title":"Activate conda on NeSI","text":"<p>First, you need to have Conda or Mamba installed on your machine.</p> <p>On NeSI, use the Miniconda environment module:</p> <pre><code>module load Miniconda3\n</code></pre> <p>and check that the <code>conda</code> tool is now available:</p> <pre><code>conda --version\n</code></pre>"},{"location":"workshop_material/supplementary/99_appendix_use_conda_environments/#run-using-the-conda-package-management-system","title":"Run using the conda package management system","text":"<p>Now let's run the example from Section 3.09 using a conda environment. First, we need to specify a conda environment for fastqc.</p> <p>Make a conda environment file for fastqc</p> <pre><code># create a folder for conda environments\nmkdir envs\n\n# create the file\ntouch ./envs/fastqc.yaml\n\n# see what versions of fastqc are available in the bioconda channel\nconda search fastqc -c bioconda\n\n# write the following to fastqc.yaml\nchannels:\n  - bioconda\n  - conda-forge\n  - defaults\ndependencies:\n  - bioconda::fastqc=0.11.9\n</code></pre> <p>This will install fastqc (version 0.11.9) from bioconda into a 'clean' conda environment separate from the rest of your computer</p> <p>See here for information on creating conda environment files.</p> <p>Update our rule to use it using the <code>conda:</code> directive, pointing the rule to the <code>envs</code> directory which has our conda environment file for fastqc (directory relative to the Snakefile)</p> <pre><code># target OUTPUT files for the whole workflow\nrule all:\n    input:\n        \"../results/fastqc/NA24631_1_fastqc.html\",\n        \"../results/fastqc/NA24631_2_fastqc.html\",\n        \"../results/fastqc/NA24631_1_fastqc.zip\",\n        \"../results/fastqc/NA24631_2_fastqc.zip\"\n\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/NA24631_1.fastq.gz\",\n        R2 = \"../../data/NA24631_2.fastq.gz\"\n    output:\n        html = [\"../results/fastqc/NA24631_1_fastqc.html\", \"../results/fastqc/NA24631_2_fastqc.html\"],\n        zip = [\"../results/fastqc/NA24631_1_fastqc.zip\", \"../results/fastqc/NA24631_2_fastqc.zip\"]\n    threads: 2\n+   conda:\n+       \"envs/fastqc.yaml\"\n   shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads}\"\n</code></pre> <p>Run again, now telling Snakemake to use to use Conda to automatically install our software by using the <code>--use-conda</code> and <code>--conda-frontend conda</code> flags</p> <pre><code># first remove output of last run\nrm -r ../results/*\n\n# Run dryrun again\n- snakemake --dryrun --cores 2\n+ snakemake --dryrun --cores 2 --use-conda --conda-frontend conda\n</code></pre> <p>My output:</p> <pre><code>Building DAG of jobs...\nConda environment envs/fastqc.yaml will be created.\nJob stats:\njob       count    min threads    max threads\n------  -------  -------------  -------------\nall           1              1              1\nfastqc        1              2              2\ntotal         2              1              2\n[Mon Sep 13 03:06:45 2021]\nrule fastqc:\n    input: ../../data/NA24631_1.fastq.gz, ../../data/NA24631_2.fastq.gz\n    output: ../results/fastqc/NA24631_1_fastqc.html, ../results/fastqc/NA24631_2_fastqc.html, ../results/fastqc/NA24631_1_fastqc.zip, ../results/fastqc/NA24631_2_fastqc.zip\n    jobid: 1\nthreads: 2\nresources: tmpdir=/dev/shm/jobs/22281190\n\n[Mon Sep 13 03:06:45 2021]\nlocalrule all:\n    input: ../results/fastqc/NA24631_1_fastqc.html, ../results/fastqc/NA24631_2_fastqc.html, ../results/fastqc/NA24631_1_fastqc.zip, ../results/fastqc/NA24631_2_fastqc.zip\n    jobid: 0\nresources: tmpdir=/dev/shm/jobs/22281190\n\nJob stats:\njob       count    min threads    max threads\n------  -------  -------------  -------------\nall           1              1              1\nfastqc        1              2              2\ntotal         2              1              2\nThis was a dry-run (flag -n). The order of jobs does not reflect the order of execution.\n</code></pre> <p>Notice it now says that \"Conda environment envs/fastqc.yaml will be created.\". Now the software our workflow uses will be automatically installed!</p> <p>Let's do a full run</p> <pre><code>- snakemake --cores 2\n+ snakemake --cores 2 --use-conda --conda-frontend conda\n</code></pre> <p>My output:</p> <pre><code>Building DAG of jobs...\nCreating conda environment envs/fastqc.yaml...\nDownloading and installing remote packages.\nEnvironment for envs/fastqc.yaml created (location: .snakemake/conda/67c1376bae89b8de73037e703ea4b6f5)\nUsing shell: /usr/bin/bash\nProvided cores: 2\nRules claiming more threads will be scaled down.\nJob stats:\njob       count    min threads    max threads\n------  -------  -------------  -------------\nall           1              1              1\nfastqc        1              2              2\ntotal         2              1              2\nSelect jobs to execute...\n\n[Mon Sep 13 03:10:27 2021]\nrule fastqc:\n    input: ../../data/NA24631_1.fastq.gz, ../../data/NA24631_2.fastq.gz\n    output: ../results/fastqc/NA24631_1_fastqc.html, ../results/fastqc/NA24631_2_fastqc.html, ../results/fastqc/NA24631_1_fastqc.zip, ../results/fastqc/NA24631_2_fastqc.zip\n    jobid: 1\nthreads: 2\nresources: tmpdir=/dev/shm/jobs/22281190\n\nActivating conda environment: /scale_wlg_persistent/filesets/project/nesi99991/snakemake20210914/lkemp/snakemake_workshop/demo_workflow/workflow/.snakemake/conda/67c1376bae89b8de73037e703ea4b6f5\nStarted analysis of NA24631_1.fastq.gz\nApprox 5% complete for NA24631_1.fastq.gz\nApprox 10% complete for NA24631_1.fastq.gz\nApprox 15% complete for NA24631_1.fastq.gz\nApprox 20% complete for NA24631_1.fastq.gz\nApprox 25% complete for NA24631_1.fastq.gz\nApprox 30% complete for NA24631_1.fastq.gz\nApprox 35% complete for NA24631_1.fastq.gz\nApprox 40% complete for NA24631_1.fastq.gz\nApprox 45% complete for NA24631_1.fastq.gz\nApprox 50% complete for NA24631_1.fastq.gz\nApprox 55% complete for NA24631_1.fastq.gz\nApprox 60% complete for NA24631_1.fastq.gz\nStarted analysis of NA24631_2.fastq.gz\nApprox 65% complete for NA24631_1.fastq.gz\nApprox 5% complete for NA24631_2.fastq.gz\nApprox 70% complete for NA24631_1.fastq.gz\nApprox 10% complete for NA24631_2.fastq.gz\nApprox 75% complete for NA24631_1.fastq.gz\nApprox 15% complete for NA24631_2.fastq.gz\nApprox 80% complete for NA24631_1.fastq.gz\nApprox 20% complete for NA24631_2.fastq.gz\nApprox 85% complete for NA24631_1.fastq.gz\nApprox 25% complete for NA24631_2.fastq.gz\nApprox 90% complete for NA24631_1.fastq.gz\nApprox 30% complete for NA24631_2.fastq.gz\nApprox 95% complete for NA24631_1.fastq.gz\nApprox 35% complete for NA24631_2.fastq.gz\nAnalysis complete for NA24631_1.fastq.gz\nApprox 40% complete for NA24631_2.fastq.gz\nApprox 45% complete for NA24631_2.fastq.gz\nApprox 50% complete for NA24631_2.fastq.gz\nApprox 55% complete for NA24631_2.fastq.gz\nApprox 60% complete for NA24631_2.fastq.gz\nApprox 65% complete for NA24631_2.fastq.gz\nApprox 70% complete for NA24631_2.fastq.gz\nApprox 75% complete for NA24631_2.fastq.gz\nApprox 80% complete for NA24631_2.fastq.gz\nApprox 85% complete for NA24631_2.fastq.gz\nApprox 90% complete for NA24631_2.fastq.gz\nApprox 95% complete for NA24631_2.fastq.gz\nAnalysis complete for NA24631_2.fastq.gz\n[Mon Sep 13 03:10:33 2021]\nFinished job 1.\n1 of 2 steps (50%) done\nSelect jobs to execute...\n\n[Mon Sep 13 03:10:33 2021]\nlocalrule all:\n    input: ../results/fastqc/NA24631_1_fastqc.html, ../results/fastqc/NA24631_2_fastqc.html, ../results/fastqc/NA24631_1_fastqc.zip, ../results/fastqc/NA24631_2_fastqc.zip\n    jobid: 0\nresources: tmpdir=/dev/shm/jobs/22281190\n\n[Mon Sep 13 03:10:33 2021]\nFinished job 0.\n2 of 2 steps (100%) done\nComplete log: /scale_wlg_persistent/filesets/project/nesi99991/snakemake20210914/lkemp/snakemake_workshop/demo_workflow/workflow/.snakemake/log/2021-09-13T030734.543325.snakemake.log\n</code></pre>"},{"location":"workshop_material/supplementary/99_appendix_use_conda_environments/#additional-information","title":"Additional information","text":"<p>Have a look at bioconda's list of packages to see the VERY extensive list of quality open source (free) bioinformatics software that is available for download and use. Note that is only one of the conda package repositories that exist, also have a look at the conda-forge and main conda package repositories.</p> <p>Another option to run your code in self-contained and reproducible environments are containers. Snakemake can use Singularity containers to execute the workflow, as detailed in the official documentation.</p>"}]}